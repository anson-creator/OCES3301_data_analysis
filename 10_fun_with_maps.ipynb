{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*30 Mar 2022, Julian Mak (whatever with copyright, do what you want with this)\n",
    "\n",
    "### As part of material for OCES 3301 \"Data Analysis in Ocean Sciences\" delivered at HKUST\n",
    "\n",
    "For the latest version of the material, go to the public facing [GitHub](https://github.com/julianmak/academic-notes/tree/master/OCES3301_data_analysis_ocean) page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you probably need this to if you run things in Colab (it is a bit slow to download by the way...)\n",
    "#!pip install cartopy  # Colab give me version \"0.19.0.post1\" it would seem\n",
    "\n",
    "# load some deafult packages\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import copy\n",
    "from scipy import signal\n",
    "from sklearn.decomposition import PCA  # not loading the StandardScaler here because it seems to screw it up...?\n",
    "\n",
    "# some modules for cartopy (I used 0.18.0 apparently, from \"cartopy.__version__\")\n",
    "import cartopy\n",
    "import cartopy.crs as ccrs  # crs = co-ordinate reference system, for the different map projections\n",
    "\n",
    "# some extra bits to modify the labels, lines and for making coluorbars\n",
    "from cartopy.mpl.gridliner import LONGITUDE_FORMATTER, LATITUDE_FORMATTER\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "# pull files from the internet if needed (e.g. temporary session in Colab)\n",
    "# !wget https://github.com/julianmak/OCES3301_data_analysis/raw/main/salinity_WOA13_decav_Reg1L46_clim.nc\n",
    "# !wget https://github.com/julianmak/OCES3301_data_analysis/raw/main/temperature_WOA13_decav_Reg1L46_clim.nc\n",
    "# !wget https://github.com/julianmak/OCES3301_data_analysis/raw/main/gebco_bathy_coarsen.nc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------\n",
    "\n",
    "# 10: Fun with maps\n",
    "\n",
    "Lets try and draw some stuff on maps. While the data I've provided are so far in regular lon/lat grids so there is no strong need to use map projections, I do not provide masks or the like to plot land with, so it might be advantageous to have a package that can plot land features, coastlines and the like. You might want to focus on the poles, which the regular lon/lat projection cannot show (here you are really trying to show a disk, but you are to showing cylinders with the lon/lat projection). There might be times when you want certain properties that simply cannot be represented in the regular lon/lat projection: for example, the area is not preserved, so giving rise to the problem that things at the equator look much smaller than they are than at the poles (e.g. India looks much smaller than Greenland, when the area is actually 1.5 times larger).\n",
    "\n",
    "Before we get to the `cartopy` package that does most of the heavy lifting for us, just a bit of (probably) useful mathematical background."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classical geometry (with some topology)\n",
    "\n",
    "Recall in `08_time_series` I introduced the notion of a basis, where a vector can be represented in different ways (with the standard canonical basis $\\{\\boldsymbol{e}_1, \\boldsymbol{e}_2\\} = \\{(1, 0), (0, 1)\\}$, or some other way basis; see example in that notebook). Similarly, we can represent positions in different ways: $(1, 1)$ could be represented as the point $x=1$ and $y=1$, but I could also represent it as $r=\\sqrt{2}$ and $\\theta=\\pi/4$, where\n",
    "\n",
    "\\begin{equation*}\n",
    "    x = r\\cos\\theta, \\qquad y = r\\sin\\theta,\n",
    "\\end{equation*}\n",
    "\n",
    "where $\\theta$ is the angle I measure from the $x$-axis, and $r$ is the radius from the origin. This is essentially a change from **Cartesian co-ordinates** to **polar co-ordinates**, and the rule given above is a **co-ordinate transformation** from $(r,\\theta)$ to $(x,y)$. See drawing below.\n",
    "\n",
    "> NOTE: A co-ordinate transformation should strictly be reversible and one-to-one. The above fails at the isolated point $r=0$ corresponding to $(x,y)=(0,0)$, since I can choose any $\\theta$ I like there, but that can be manually patched out, so it can be regarded as a co-ordinate transformation.\n",
    "\n",
    "<img src=\"https://i.imgur.com/VFzpEO2.png\" width=\"400\" alt='cursed coordinate'>\n",
    "\n",
    "Again, there is nothing particularly mysterious about this, it's just representing things in a different (and perhaps more natural) way. For example, the function corresponding to the circle of radius one in Cartesian co-ordinates is (trust me on this, or try this yourselves with some coding)\n",
    "\n",
    "\\begin{equation*}\n",
    "    y = \\pm \\sqrt{1 - x^2}\n",
    "\\end{equation*}\n",
    "\n",
    "for $x$ running from $-1$ to $1$ (and you need the plus or minus to draw the two segments separately). On the other hand, in polar co-ordinates this is simply\n",
    "\n",
    "\\begin{equation*}\n",
    "    r = 1\n",
    "\\end{equation*}\n",
    "\n",
    "for $\\theta$ running from $0$ to $2\\pi$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem of co-ordinate system is particularly notable if you are talking about data on a sphere. If you are on the surface of the sphere, generically you will have to specify a ($x,y,z$), but if you take for example **spherical co-ordinates** ($r, \\theta, \\phi$), which corresponds here respectively to radius, longitude and latitude, then if we are on the surface of the Earth, then we set $r = R_{\\rm Earth}$, and we are only left with longitude and latitude, i.e. we need to carry one less variable around. If we only have two variables, then you can represent it as a 2d plot, which is of course easier (cf. `04_multilinear_regression`, where you use a PCA to get lower dimensional data).\n",
    "\n",
    "> NOTE: I'll leave you to look up the transformation rules between Cartesian and spherical co-ordinates. You will need this if you need to compute the **metric** terms to get **distances**, which is part of some extended exercises here and in the previous notebook.\n",
    "\n",
    "In principle you can plot data gridded at lon/lat as a flat 2d, but there is a limitation: the surface of the sphere (a 2-sphere or $S^2$) is fundamentally different to that of the plane, since the former is clearly curved while the other is flat. What this means is that the *topology* is fundamentally different (in this case through the Euler characteristic, which is related to the *Gaussian curvature* through the [Gauss-Bonnet theorem](https://en.wikipedia.org/wiki/Gauss%E2%80%93Bonnet_theorem); not going to elaborate on that). Topology is perhaps best explained through the classic example of a (cursed?) spherical cow and coffee cup / doughtnut:\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/2/24/Spot_the_cow.gif?20211112013016\" width=\"200\" alt='cursed cow'><img src=\" https://c.tenor.com/W13M_b0FkVMAAAAd/topology-cup.gif\" width=\"200\" alt='coffee cup'>\n",
    "\n",
    "Here we can smoothly deform the cow into the sphere, and the cup into the doughnut, without \"tearing\" the surface. In that sense, they are respectively topologically equivalent, but the cow is topologically distinct to the doughnut (the topological quantity here is the *genus*, which you could think of as \"holes\"). \n",
    "\n",
    "Going back to the sphere case which is our main interest, since the sphere is topologically distinct to the plane, you have to introduce a \"tear\" to deform the sphere onto the plane (you actually only need to remove one point). The practical consequence here is that there is ***no way to preserve both areas and angles when doing planar map projections of spherical data***. You can mathematically prove this; there is no *isometry* (a distance preserving map) between the sphere and the Cartesian plane. What this means is any **map projection** that takes your lon/lat to ($x,y$) can at best:\n",
    "\n",
    "1) preserve angles (*conformal mapping*; e.g. Mercator, Lambert Conformal)\n",
    "\n",
    "2) preserve areas (*area-preserving*; e.g. Mollweide, Interrupted Goode Homolosine)\n",
    "\n",
    "3) neither of those (e.g. Plate Carree, Robinson, Orthographic)\n",
    "\n",
    "(An isometry does both 1 and 2.) The regular lon/lat projection is a case where you do neither. Sometimes you may or may not want some of these properties. There are multiple choices in how you choose the represent the lon/lat data on a flat space, given by various map projections, each with their own mathematical formula to translate from lon/lat to some ($x,y$) to give you the representation on the flat plane; see for example the [Wikipedia page](https://en.wikipedia.org/wiki/Map_projection)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------\n",
    "## a) `cartopy`\n",
    "\n",
    "The spiel above is mostly driven by the fact I really wanted to include the spherical cow...anyway, the upshot here is that a lot of heavy lifting (particularly the maths) has actually been coded up and is already available in the package `cartopy` for example. I am going to demonstrate syntax with examples below; the way you might want to do this is to see what you like, and try and fish out the code that gives you the things you like. If there are other things you want to do, Google and Stack exchange/overflow is your friend (most of the time).\n",
    "\n",
    "The bit of code below shows a selection of map projections available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of \"common\" projections\n",
    "# code adapted from Ryan Abernathey (https://rabernat.github.io/research_computing_2018/maps-with-cartopy.html)\n",
    "\n",
    "projections = [ccrs.PlateCarree(),\n",
    "               ccrs.Robinson(),\n",
    "               ccrs.Mercator(),\n",
    "               ccrs.Orthographic(),                                            # default center at (0,0)\n",
    "               ccrs.Orthographic(central_latitude=90),                         # northern orthographic\n",
    "               ccrs.Orthographic(central_longitude=90, central_latitude=-90),  # southern orthographic with shift\n",
    "               ccrs.Mollweide(),\n",
    "               ccrs.LambertConformal(),\n",
    "               ccrs.InterruptedGoodeHomolosine()\n",
    "              ]\n",
    "\n",
    "fig = plt.figure(figsize=(16, 13))\n",
    "\n",
    "for i, proj in enumerate(projections):\n",
    "    ax = plt.subplot(3, 3, i+1, projection=proj)\n",
    "    ax.stock_img()\n",
    "    ax.coastlines()\n",
    "    ax.set_title(f\"{type(proj)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets try it on some data. Below code loads the WOA13 surface (conservative) temperature data introduced last time via xarray, and then does a basic plot using the Plate Carree projection. The main addition here are:\n",
    "\n",
    "* the `projection=CARTOPY_PROJECTION` to the axes object, which turns regular matplotlib axes into a cartopy axes with added attributes that can be modified\n",
    "\n",
    "* `ax.add_feature(cartopy.feature.LAND)` which adds land features onto the map\n",
    "  * if you are running this in Colab, it will download the files on the fly, but if you exist Colab you will have to re-download it on the fly next time since the packages/data are not cached\n",
    "\n",
    "> NOTE: Instead of repeatedly called `ccrs.PlateCarree()` I just defined it as `pcarree = ccrs.PlateCarree()`. The reason is that the associated object is used in various places even if when we are not dealing with the Plate Carree projection itself. See examples later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the temperature and salinity profiles\n",
    "toce_WOA13 = xr.open_dataset(\"temperature_WOA13_decav_Reg1L46_clim.nc\")\n",
    "\n",
    "lon, lat = toce_WOA13[\"lon\"], toce_WOA13[\"lat\"]\n",
    "toce = toce_WOA13[\"votemper\"].isel(lev=0)  # only pick out the surface data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the axes with projections\n",
    "pcarree = ccrs.PlateCarree()\n",
    "\n",
    "# define some plot options\n",
    "plot_opts = {\"cmap\"   : \"RdBu_r\",\n",
    "             \"levels\" : np.linspace(0, 30, 31),\n",
    "             \"extend\" : \"both\"}\n",
    "\n",
    "fig = plt.figure(figsize=(14, 5))\n",
    "\n",
    "kt = 0\n",
    "ax = plt.subplot(1, 2, 1, projection=pcarree)\n",
    "ax.contourf(lon, lat, toce.isel(time=kt), **plot_opts)\n",
    "ax.add_feature(cartopy.feature.LAND, zorder = 10, edgecolor = 'k')\n",
    "ax.set_title(f\"$\\Theta(z=0, t={toce.time[kt].values})$\")\n",
    "\n",
    "kt = 5\n",
    "ax = plt.subplot(1, 2, 2, projection=ccrs.PlateCarree())\n",
    "ax.contourf(lon, lat, toce.isel(time=kt), **plot_opts)\n",
    "ax.add_feature(cartopy.feature.LAND, zorder = 10, edgecolor = 'k')\n",
    "ax.set_title(f\"$\\Theta(z=0, t={toce.time[kt].values})$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The maps above could do with a bit more decoration, because for example it is not very obvious how different the January data (left) is to the June data (right). The decorations below are:\n",
    "\n",
    "* `gl=ax.gridlines` for gridlines\n",
    "  * modifications to grid lines through modifying the `gl` object attributes\n",
    "\n",
    "* adding axes labels as `ax.text` (`ax.set_xlabel` behaves differently for the cartopy axes)\n",
    "\n",
    "* `divider = make_axes_locatable(ax)` in preparation for a colour bar (this is a native `matplotlib` command)\n",
    "  * done this way to make sure colour bar scales with the plot size, otherwise the bar and panel size do not match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# as above but with a bit more decorations\n",
    "\n",
    "fig = plt.figure(figsize=(14, 5))\n",
    "\n",
    "kt = 0\n",
    "ax = plt.subplot(1, 2, 1, projection=pcarree)\n",
    "ax.contourf(lon, lat, toce.isel(time=kt), **plot_opts)\n",
    "ax.add_feature(cartopy.feature.LAND, zorder = 10, edgecolor = 'k')\n",
    "ax.set_title(f\"$\\Theta(z=0, t={toce.time[kt].values})$\")\n",
    "\n",
    "gl = ax.gridlines(crs=pcarree, draw_labels=True,\n",
    "                  linewidth=2, color='gray', alpha=0.5, linestyle='--')\n",
    "gl.top_labels = False\n",
    "gl.right_labels = False\n",
    "ax.set_extent([-180, 180, -80, 90], crs=pcarree)\n",
    "ax.text(0.5, -0.20, r'Lon $\\left( {}^\\circ \\right)$',\n",
    "        va='bottom', ha='center',\n",
    "        rotation='horizontal', rotation_mode='anchor',\n",
    "        transform=ax.transAxes)\n",
    "ax.text(-0.12, 0.5, r'Lat $\\left( {}^\\circ \\right)$',\n",
    "        va='bottom', ha='center',\n",
    "        rotation=90, rotation_mode='anchor',\n",
    "        transform=ax.transAxes)\n",
    "\n",
    "kt = 5\n",
    "ax = plt.subplot(1, 2, 2, projection=pcarree)\n",
    "cs = ax.contourf(lon, lat, toce.isel(time=kt), **plot_opts)  # define cs for making the colorbar\n",
    "ax.add_feature(cartopy.feature.LAND, zorder = 10, edgecolor = 'k')\n",
    "ax.set_title(f\"$\\Theta(z=0, t={toce.time[kt].values})$\")\n",
    "\n",
    "gl = ax.gridlines(crs=pcarree, draw_labels=True,\n",
    "                  linewidth=2, color='gray', alpha=0.5, linestyle='--')\n",
    "gl.top_labels = False\n",
    "gl.right_labels = False\n",
    "gl.left_labels = False\n",
    "ax.set_extent([-180, 180, -80, 90], crs=pcarree)\n",
    "ax.text(0.5, -0.20, r'Lon $\\left( {}^\\circ \\right)$',\n",
    "        va='bottom', ha='center',\n",
    "        rotation='horizontal', rotation_mode='anchor',\n",
    "        transform=ax.transAxes)\n",
    "\n",
    "divider = make_axes_locatable(ax)  # add a colorbar\n",
    "cb_ax = divider.append_axes(\"right\", size = \"2%\", pad = 0.2, axes_class=plt.Axes)\n",
    "cax = plt.colorbar(cs, cax=cb_ax)\n",
    "cax.set_ticks([0, 5, 10, 15, 20, 25, 30])\n",
    "cax.ax.set_title(r\"$^\\circ\\mathrm{C}$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below shows zooms and so forth, still sticking with the plate Carree projection. Some comments:\n",
    "\n",
    "* one could set the zoom by `ax.set_extent` if you are only dealing with `contourf`\n",
    "\n",
    "* when combined with contours I don't find the results very satisfactory, so what I did was to select the data I want through indexing (done via xarray), and then plot the data, instead of relying on `ax.set_extent`\n",
    "\n",
    "* I did the `contourf` results first and then overlaid the `lines=contour` results to show some labelled contour lines (`ax.clabel(lines, inline=True)`), using the same subsetting procedure as above\n",
    "  * if you rely on `ax.set_extent` then the labels may not show in your zoomed in region, and the inline labelling might look a bit weird"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zoom in to various places\n",
    "\n",
    "fig = plt.figure(figsize=(14, 5))\n",
    "\n",
    "kt = 0\n",
    "\n",
    "plot_opts = {\"cmap\"   : \"RdBu_r\",\n",
    "             \"levels\" : np.linspace(0, 30, 31),\n",
    "             \"extend\" : \"both\"}\n",
    "\n",
    "# define limits to do things over: North Atlantic here\n",
    "lon_lim, lat_lim = (-80, 10), (0, 65)\n",
    "\n",
    "ax = plt.subplot(1, 2, 1, projection=pcarree)\n",
    "\n",
    "# note: the contour labels and gridlines are nicer if you force a plot based on the sliced out data\n",
    "#       (another way to do it is to set the extent, see the commented line below)\n",
    "ax.contourf(lon.sel(lon=slice(lon_lim[0], lon_lim[1])), \n",
    "            lat.sel(lat=slice(lat_lim[0], lat_lim[1])), \n",
    "            toce.isel(time=kt).sel(lon=slice(lon_lim[0], lon_lim[1]), lat=slice(lat_lim[0], lat_lim[1])),\n",
    "            **plot_opts)\n",
    "lines = ax.contour(lon.sel(lon=slice(lon_lim[0], lon_lim[1])), \n",
    "                   lat.sel(lat=slice(lat_lim[0], lat_lim[1])), \n",
    "                   toce.isel(time=kt).sel(lon=slice(lon_lim[0], lon_lim[1]), lat=slice(lat_lim[0], lat_lim[1])), \n",
    "                   colors=\"k\", levels=np.arange(5, 31, 5))\n",
    "ax.clabel(lines, inline=True)\n",
    "ax.add_feature(cartopy.feature.LAND, zorder = 10, edgecolor = 'k')\n",
    "ax.set_title(f\"$\\Theta(z=0, t={toce.time[kt].values})$\")\n",
    "\n",
    "gl = ax.gridlines(crs=pcarree, draw_labels=True,\n",
    "                  linewidth=2, color='gray', alpha=0.5, linestyle='--')\n",
    "gl.top_labels = False\n",
    "gl.right_labels = False\n",
    "# ax.set_extent([lon_lim[0], lon_lim[1], lat_lim[0], lat_lim[1]], crs=pcarree)\n",
    "ax.text(0.5, -0.20, r'Lon $\\left( {}^\\circ \\right)$',\n",
    "        va='bottom', ha='center',\n",
    "        rotation='horizontal', rotation_mode='anchor',\n",
    "        transform=ax.transAxes)\n",
    "ax.text(-0.12, 0.5, r'Lat $\\left( {}^\\circ \\right)$',\n",
    "        va='bottom', ha='center',\n",
    "        rotation=90, rotation_mode='anchor',\n",
    "        transform=ax.transAxes)\n",
    "\n",
    "# define limits to do things over: South China Sea + Pacific\n",
    "lon_lim, lat_lim = (100, 140), (0, 45)\n",
    "\n",
    "plot_opts = {\"cmap\"   : \"RdBu_r\",\n",
    "             \"levels\" : np.linspace(10, 30, 30),\n",
    "             \"extend\" : \"both\"}\n",
    "\n",
    "ax = plt.subplot(1, 2, 2, projection=pcarree)\n",
    "cs = ax.contourf(lon.sel(lon=slice(lon_lim[0], lon_lim[1])), \n",
    "                 lat.sel(lat=slice(lat_lim[0], lat_lim[1])),\n",
    "                 toce.isel(time=kt).sel(lon=slice(lon_lim[0], lon_lim[1]), lat=slice(lat_lim[0], lat_lim[1])),\n",
    "                 **plot_opts)  # define cs for making the colorbar\n",
    "lines = ax.contour(lon.sel(lon=slice(lon_lim[0], lon_lim[1])),\n",
    "                   lat.sel(lat=slice(lat_lim[0], lat_lim[1])),\n",
    "                   toce.isel(time=kt).sel(lon=slice(lon_lim[0], lon_lim[1]), lat=slice(lat_lim[0], lat_lim[1])),\n",
    "                   colors=\"k\", levels=np.arange(10, 30, 2))\n",
    "ax.clabel(lines, inline=True)\n",
    "ax.add_feature(cartopy.feature.LAND, zorder = 10, edgecolor = 'k')\n",
    "ax.set_title(f\"$\\Theta(z=0, t={toce.time[kt].values})$\")\n",
    "\n",
    "gl = ax.gridlines(crs=pcarree, draw_labels=True,\n",
    "                  linewidth=2, color='gray', alpha=0.5, linestyle='--')\n",
    "gl.top_labels = False\n",
    "gl.right_labels = False\n",
    "# ax.set_extent([lon_lim[0], lon_lim[1], lat_lim[0], lat_lim[1]], crs=pcarree)\n",
    "ax.text(0.5, -0.20, r'Lon $\\left( {}^\\circ \\right)$',\n",
    "        va='bottom', ha='center',\n",
    "        rotation='horizontal', rotation_mode='anchor',\n",
    "        transform=ax.transAxes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the below I do something similar but for the GEBCO bathymetry data and showing these in different map projections (the Mercator and North Pole centered orthographic). Comments:\n",
    "\n",
    "* the different projections are achieved by changing the `projection` keyword in the axes object\n",
    "\n",
    "* note the addition in this case of `transform=pcarree` in the `contourf` commands\n",
    "  * no I don't know why the transform is pcarree when the projection is Mercator, if you find out tell me...\n",
    "  \n",
    "* I downsized the data because any transform away from standard pcarree can take some time (I think this is especially true when the orthographic projections are concerned)\n",
    "  * downsizing by a factor of 4 (`[::4]`) seems to be alright, but you will notice it still takes a bit of time to show the results\n",
    "  * downsizing by anything larger than 6 I found the results started to look a bit too smudged and losing details, but have an experiment with this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting bathymetry but in a different projection\n",
    "\n",
    "bathy = xr.open_dataset(\"gebco_bathy_coarsen.nc\")\n",
    "\n",
    "fig = plt.figure(figsize=(14, 5))\n",
    "\n",
    "plot_opts = {\"cmap\"   : \"gist_earth\",\n",
    "             \"levels\" : np.linspace(-5000, 5000, 30),\n",
    "             \"extend\" : \"both\"}\n",
    "\n",
    "# mercator projection\n",
    "ax = plt.subplot(1, 2, 1, projection=ccrs.Mercator())\n",
    "ax.contourf(bathy[\"lon\"][::4], \n",
    "            bathy[\"lat\"][::4], \n",
    "            bathy[\"elev\"][::4, ::4], \n",
    "            transform=pcarree, **plot_opts)  # \n",
    "ax.add_feature(cartopy.feature.LAND, zorder = 10, edgecolor = 'k')\n",
    "ax.set_title(f\"elevation from GEBCO\")\n",
    "\n",
    "gl = ax.gridlines(crs=pcarree, draw_labels=True,\n",
    "                  linewidth=2, color='gray', alpha=0.5, linestyle='--')\n",
    "gl.top_labels = False\n",
    "gl.right_labels = False\n",
    "ax.set_extent([-180, 180, -80, 90], crs=pcarree)\n",
    "ax.text(0.5, -0.20, r'Lon $\\left( {}^\\circ \\right)$',\n",
    "        va='bottom', ha='center',\n",
    "        rotation='horizontal', rotation_mode='anchor',\n",
    "        transform=ax.transAxes)\n",
    "ax.text(-0.12, 0.5, r'Lat $\\left( {}^\\circ \\right)$',\n",
    "        va='bottom', ha='center',\n",
    "        rotation=90, rotation_mode='anchor',\n",
    "        transform=ax.transAxes)\n",
    "\n",
    "# northern orthographic (sometimes northern stereographic)\n",
    "ax = plt.subplot(1, 2, 2, projection=ccrs.Orthographic(central_longitude=0, central_latitude=90))\n",
    "cs = ax.contourf(bathy[\"lon\"][::4], \n",
    "                 bathy[\"lat\"][::4], \n",
    "                 bathy[\"elev\"][::4, ::4], \n",
    "                 transform=pcarree, **plot_opts)  # define cs for making the colorbar\n",
    "ax.add_feature(cartopy.feature.LAND, zorder = 10, edgecolor = 'k')\n",
    "ax.set_title(f\"elevation from GEBCO\")\n",
    "ax.set_global()  # NOTE: seem to need this magic line to display the plot in this projection\n",
    "\n",
    "gl = ax.gridlines(crs=pcarree, draw_labels=True,\n",
    "                  linewidth=2, color='gray', alpha=0.5, linestyle='--')\n",
    "\n",
    "divider = make_axes_locatable(ax)  # add a colorbar\n",
    "cb_ax = divider.append_axes(\"right\", size = \"2%\", pad = 0.2, axes_class=plt.Axes)\n",
    "cax = plt.colorbar(cs, cax=cb_ax)\n",
    "cax.ax.set_title(r\"$\\mathrm{m}$\")\n",
    "cax.set_ticks([-5000, -2500, 0, 2500, 5000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <span style=\"color:red\">**Q.**</span> Have some fun with these, maybe make some movies with the data or whatever. Ask me if you want some data like `current_speed.nc` below but over a different region to generate movies with (e.g. data just off the Drake passage in the Southern Ocean for example)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------\n",
    "# b) Power spectrum in space and/or time\n",
    "\n",
    "Recall that in *08_time_series* we talked about Fourier modes, applied to a time-series, to get a power spectrum (well-defined since Fourier modes is an orthogonal basis in the relevant function space) in terms of frequency/periods, and do appropriate filtering on those by modifying the spectrum. The routine we rely on is `np.fft.(r)fft` (and `np.fft.i(r)fft` as appropriate), but they are also provided by other packages (e.g. `scipy.fft.(r)fft`).\n",
    "\n",
    "Recall also that the Fourier transform just takes in a set of numbers and gives you the Fourier amplitudes back, and makes no reference as such to the input time dimension itself, so in principle you could apply the same techniques to data with a *space* variable. Recapping some of this for use later with EOFs, instead of\n",
    "\n",
    "\\begin{equation*}\n",
    "    f(t) = a_0 + \\sum_{k=1}^N a_k \\cos(kt) + \\sum_{j=1}^N b_j \\sin(jt),\n",
    "\\end{equation*}\n",
    "where our basis function is for the time dimension and the amplitudes are just numbers, we might consider instead\n",
    "\n",
    "\\begin{equation*}\n",
    "    f(x,t) = a_0(t) + \\sum_{k=1}^N a_k(t) \\cos(kx) + \\sum_{j=1}^N b_j(t) \\sin(jx),\n",
    "\\end{equation*}\n",
    "\n",
    "where our basis function is now for the *space* dimension, and the amplitudes are now functions of $t$. The procedure is exactly the same, and the only change we would do is to re-scale and interpret the *wavenumber* differently, and the power spectrum in this case would be in terms of wavenumber/wavelengths, which would be time-evolving in principle, and we could filter *in space* accordingly.\n",
    "\n",
    "The below code does this for the `current_speed.nc` file that we used in `09_fun_with_maps`, first by applying the Fourier analysis on a time-series (function of $t$ only), for a slice varying in space (function of $x$ only), and then something slightly more complicated for a horizontal slice (function of $x$ and $y$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time-series (at a fixed lon and lat)\n",
    "\n",
    "This is as before in `08_time_series`, but for semi-realistic data. I am going to just select a point (marked on by a triangle below), pick out the time-series (through `xarray`), and do the usual analysis on the resulting time-series. This would be like the case where you have a buoy or a mooring in the ocean collecting data. First I show what the data actually looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# power spectrum of downsized data\n",
    "\n",
    "df = xr.open_dataset(\"current_speed.nc\")\n",
    "lon_ind, lat_ind = 250, 250\n",
    "\n",
    "f = df[\"speed\"].isel(lat=lat_ind, lon=lon_ind).values\n",
    "t = np.arange(len(f)) * 5.0 + 2.5 # turn index into units of DAYS, with a shift (corresponds to time-centers)\n",
    "\n",
    "fig = plt.figure(figsize=(12, 4))\n",
    "ax = plt.subplot(1, 2, 1)\n",
    "df[\"speed\"].isel(time=0).plot(ax=ax)\n",
    "ax.plot(df[\"lon\"][lon_ind], df[\"lat\"][lat_ind], 'C3^', markersize=12)\n",
    "ax.grid()\n",
    "\n",
    "ax = plt.subplot(1, 2, 2)\n",
    "ax.plot(t, f, 'C0-x')\n",
    "ax.set_xlabel(r\"$t$ (days)\")\n",
    "ax.set_ylabel(r\"$|u|^2\\ (\\mathrm{m}\\ \\mathrm{s}^{-1})$\")\n",
    "ax.set_title(f\"lon$={df['lon'][lon_ind].values:.2f}^\\circ$, lat$={df['lat'][lat_ind].values:.2f}^\\circ$\")\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking a FFT of the time-series, I can compute the power spectrum. Beyond having to re-scale the wavenumber (or really angular frequency), and convert accordingly to frequencies and periods, the code below is essentially a copy and paste job from `08_time_series`.\n",
    "\n",
    "> NOTE: Here my time units are in DAYS, and I know in this case the output is every 5 days, so I need to work out the elapsed time and rescale the `k_vec` accordingly. I am going to skip the zero mode for simplicity, not bother windowing the data and forcing the data to be periodic, and not normalising the power spectrum (because I am only interested in the shape)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# power spectrum analysis at one point (things given in FREQUENCIES + PERIODS etc.)\n",
    "\n",
    "f_h = np.fft.rfft(f)\n",
    "\n",
    "# remember we need to rescale things (by length of time elapsed)\n",
    "L = t[-1] - t[0]\n",
    "scale_factor = 2.0 * np.pi / L\n",
    "k_vec = np.arange(len(f_h)) * scale_factor\n",
    "\n",
    "fig = plt.figure(figsize=(14, 3))\n",
    "ax = plt.subplot(1, 3, 1)\n",
    "ax.plot(k_vec[1::], abs(f_h[1::])**2, 'C0o-')\n",
    "ax.set_xlabel(r\"$k$ (rad per day)\")\n",
    "ax.set_ylabel(r\"$|\\hat{f}|^2$\")\n",
    "ax.set_title(r\"angular frequency\")\n",
    "ax.grid()\n",
    "\n",
    "ax = plt.subplot(1, 3, 2)\n",
    "ax.plot(k_vec[1::] / (2.0*np.pi), abs(f_h[1::])**2, 'C0o-')\n",
    "ax.set_xlabel(r\"$\\nu$ (day$^{-1}$)\")\n",
    "ax.set_title(r\"frequency\")\n",
    "ax.grid()\n",
    "\n",
    "ax = plt.subplot(1, 3, 3)\n",
    "ax.plot(2.0*np.pi / k_vec[1::], abs(f_h[1::])**2, 'C0o-')\n",
    "ax.set_xlabel(r\"$T$ (day)\")\n",
    "ax.set_title(r\"period\")\n",
    "ax.grid()\n",
    "\n",
    "fig.tight_layout(pad=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we see here is that most of the power is in the lower frequencies, or the longer periods. The power in the roughly one year, half a year and quarter of a year signal might be attributed to a seasonal cycle (although that might be a bit dubious to claim since we only have one year of data), but notice there is also non-negligible variability in the sub-60 day region, which presumably corresponds to the mesoscale activity from the eddies. Here they might be interpreted as mesoscale eddies passing through the point.\n",
    "\n",
    "> <span style=\"color:red\">**Q.**</span> Try doing something similar but for other points. In particular, try doing this but picking points that are further away from the jet in the more quiescent regions.\n",
    "\n",
    "> <span style=\"color:red\">**Q.**</span> Consider filtering out the signal (e.g. low pass filters) to get rid of the things that might be attributed to the seasonal cycle and see what you end up with. Again try this for other locations.\n",
    "\n",
    "> <span style=\"color:red\">**Q.**</span> Consider doing a collection of power spectra at different locations and take an \"average\" of sorts, since doing a power spectrum on just one point and then making conclusions for the general domain is arguably a bit dubious."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spatial track (function of longitude here, fixed time and latitude)\n",
    "\n",
    "As above, but I am going to fix the time and latitude, and pull out a horizontal track (cf. \"track\" from ship track). Again, plot out the data first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# could do it in a spatial slice per time\n",
    "\n",
    "f = df[\"speed\"].isel(time=0, lat=lat_ind).values\n",
    "\n",
    "fig = plt.figure(figsize=(12, 4))\n",
    "ax = plt.subplot(1, 2, 1)\n",
    "df[\"speed\"].isel(time=0).plot(ax=ax)\n",
    "ax.plot([df[\"lon\"][0], df[\"lon\"][-1]], [df[\"lat\"][lat_ind], df[\"lat\"][lat_ind]], 'C3--', linewidth=4)\n",
    "ax.grid()\n",
    "\n",
    "ax = plt.subplot(1, 2, 2)\n",
    "ax.plot(df[\"lon\"], f, 'C0-x')\n",
    "ax.set_xlabel(r\"$t$ (days)\")\n",
    "ax.set_ylabel(r\"$|u|^2\\ (\\mathrm{m}\\ \\mathrm{s}^{-1})$\")\n",
    "ax.set_title(f\"lat$={df['lat'][lat_ind].values:.2f}^\\circ$\")\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below is again a copy and paste job, except here we need to rescale the `k_vec` according to the length.\n",
    "\n",
    "> NOTE: For convenience I took a conversion that 1 degree is 100 km, so now my variables should be interpreted as wavenumbers and wavelength with the length unit as kilometers. Again I am going to skip the zero mode, not bothering to window the data, and not normalising the power spectrum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# power spectrum analysis at one slice (things given in WAVENUMBERS + WAVELENGTH etc.)\n",
    "\n",
    "f_h = np.fft.rfft(f)\n",
    "\n",
    "# remember we need to rescale things (by length of domain in longtiude in this case)\n",
    "# need to do a bit more work and modify the length here; for simplicity 1 deg = 100 km\n",
    "\n",
    "L = (df[\"lon\"][-1] - df[\"lon\"][0]).values * 100  # in units of km\n",
    "scale_factor = 2.0 * np.pi / L\n",
    "k_vec = np.arange(len(f_h)) * scale_factor\n",
    "\n",
    "fig = plt.figure(figsize=(14, 3))\n",
    "ax = plt.subplot(1, 3, 1)\n",
    "ax.semilogx(k_vec[1::], abs(f_h[1::])**2, 'C0o-')\n",
    "ax.set_xlabel(r\"$k$ (rad per km)\")\n",
    "ax.set_ylabel(r\"$|\\hat{f}|^2$\")\n",
    "ax.set_title(r\"wavenumber\")\n",
    "ax.grid()\n",
    "\n",
    "ax = plt.subplot(1, 3, 2)\n",
    "ax.semilogx(k_vec[1::] / (2.0*np.pi), abs(f_h[1::])**2, 'C0o-')\n",
    "ax.set_xlabel(r\"$\\nu$ (km$^{-1}$)\")\n",
    "ax.set_title(r\"frequency\")\n",
    "ax.grid()\n",
    "\n",
    "ax = plt.subplot(1, 3, 3)\n",
    "ax.semilogx(2.0*np.pi / k_vec[1::], abs(f_h[1::])**2, 'C0o-')\n",
    "ax.set_xlabel(r\"$\\lambda$ (km)\")\n",
    "ax.set_title(r\"wavelength\")\n",
    "ax.grid()\n",
    "\n",
    "fig.tight_layout(pad=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As above, we have a notable variability on the low wavenumbers and the large wavelengths, i.e. the large spatial scales, which is probably associated with the main jet meandering around. The smaller fluctuations at a scale between 100 to 1000 km are presumably to do with the smaller eddies floating through the horizontal track; note however that mesoscale eddies might be expected to be around the 100 km length scale.\n",
    "\n",
    "> <span style=\"color:red\">**Q.**</span> As above, try different tracks, such as those varying in longitude as well as latitude, and those that cut the jet in fewer places.\n",
    "\n",
    "> <span style=\"color:red\">**Q.**</span> Try also changing the time and doing a similar investigation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Horizontal slice (fixed time, but varying in longitude and latitude)\n",
    "\n",
    "So far we have been doing FFTs in 1d but we could do it in 2d too. In the below code I am computing the power spectrum in 2d: the thing I do have to be a bit careful about is to rescale the lengths in the different directions (the latitude spacing is slightly different to the longitude spacing).\n",
    "\n",
    "> NOTE: I have been using `np.fft.rfft` which automatically chops off half of the Fourier amplitudes that I don't actually need (if the signal is real then the result is symmetric about $k=0$ and I don't really need it). If you were to do the FFT in two dimensions (rows and columns one after the other) then `rfft` would complain, because remember a FFT results in *complex* numbers, and your signal is no longer purely real after the first FFT. The way I am doing it below is to use the full `fft`, and then manually throwing away the parts I don't need (via the `Nx//2` and `Ny//2` indexing below, so only keeping the first $N/2$ entries in each dimension).\n",
    "\n",
    "> NOTE: I am also using the `pcolormesh` plotting command (pseudo-coloring over a mesh), which gives me all the raw details that are not necessarily present if `contourf` is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do this in 2d?\n",
    "\n",
    "f = df[\"speed\"].isel(time=0)\n",
    "\n",
    "# doing the full thing, but only need the first half in each dimension\n",
    "f_h = np.fft.fft(np.fft.fft(f, axis=0), axis=1)\n",
    "\n",
    "# modify both lon and lat space\n",
    "Nx, Ny = f_h.shape\n",
    "\n",
    "Lx = (df[\"lon\"][-1] - df[\"lon\"][0]).values * 100  # in units of km\n",
    "scale_factor_x = 2.0 * np.pi / Lx\n",
    "kx_vec = np.arange(Nx//2) * scale_factor_x\n",
    "\n",
    "Ly = (df[\"lat\"][-1] - df[\"lat\"][0]).values * 100  # in units of km\n",
    "scale_factor_y = 2.0 * np.pi / Ly\n",
    "ky_vec = np.arange(Ny//2) * scale_factor_y\n",
    "\n",
    "# do the plot\n",
    "fig = plt.figure(figsize=(6, 4))\n",
    "ax = plt.axes()\n",
    "cs = ax.pcolormesh(kx_vec, ky_vec, np.log10(abs(f_h[:Nx//2, :Ny//2])**2), \n",
    "                   cmap=\"RdBu_r\", shading=\"auto\", vmin=-4, vmax=4)\n",
    "ax.set_xlabel(r\"$k_x$\")\n",
    "ax.set_ylabel(r\"$k_y$\")\n",
    "ax.set_title(\"2d spatial power spectrum (not normalised)\")\n",
    "\n",
    "cb = plt.colorbar(cs)\n",
    "cb.set_ticks([-4, -2, 0, 2, 4])\n",
    "cb.set_ticklabels([r\"$10^{-4}$\", r\"$10^{-2}$\", r\"$10^0$\", r\"$10^2$\", r\"$10^4$\"])\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here what we see is that most of the power is in the low wavenumbers (so the long wavelengths), presumably related to that massive jet we have through the domain which has a large spatial scale associated with it. One interesting thing to see is that there are distinct horizontal/vertical bands of power, which would probably correspond to the jet (the longitudinal and latitudinal slices). \n",
    "\n",
    "The power radiates out from the zero modes somewhat radially, but note that the two axes are on a different scale, and in this case arguably (by the rigourous method of eyeing the spectrum...) the power diminishes faster in $k_x$ than $k_y$, i.e. there are more variations in the latitudinal direction than longitudinal direction. This is consistent in this case with the jet predominantly flowing in the zonal direction, and less in the meridional direction. What we have at the intermediate $k_x$ and $k_y$ are presumably the mesoscale eddies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filtering in 2d\n",
    "\n",
    "We can do filtering in 2d as in the 1d case. The useful quantity to consider here is filtering by a length-scale, and the thing we have to be a little bit careful about is to impose a conditional based on the length-scale, which means we need to convert the wavenumber to a wavelength.\n",
    "\n",
    "We need to be a bit careful about this to make sure we are modifying the spectrum appropriately. In the 1d case we were working directly with the Fourier coefficients based on $k=0, \\ldots, \\rm{int}(N/2)$, since our signal is real and `rfft` throws away the right amount of entries for us. But since we are in 2d and dealing with the full `fft` command, we would have to be careful and construct the wavenumber vector\n",
    "\n",
    "\\begin{equation*}\n",
    "    k_x=0, \\ldots, \\rm{int}(N/2), -\\rm{int}(N/2-1), \\ldots, -2, -1\n",
    "\\end{equation*}\n",
    "\n",
    "manually, and analogously for $k_y$, and re-scale them accordingly by a unit of length. Then, what we want to do is to compute \n",
    "\n",
    "\\begin{equation*}\n",
    "    k = |\\boldsymbol{k}| = \\sqrt{k_x^2 + k_y^2},\n",
    "\\end{equation*}\n",
    "\n",
    "which assigns to each entry in a 2d array the (positive-definite) value of $k$, which can be converted to a length via rescaling (in this case simply doing $2\\pi/k$.\n",
    "\n",
    "The reason for doing this is we essentially want to modify the spectrum based on some sort of circle with radius (in spectral space) measured from the origin where the zero mode is based, and we want to keep modes inside some radius if we want a low-pass, and keep modes outside if we want a high pass. If we instead filter based on $k_x$ and $k_y$ separately, what we end up doing is to filter based on a rectangle, which would keep too many/few modes if we are low/high passing.\n",
    "\n",
    "So, the plan of attack is:\n",
    "\n",
    "1) given the re-scaled $k_x$ and $k_y$, work out the 2d array for $|k|$\n",
    "\n",
    "2) do $2\\pi/|k|$, which returns the wavelength as a 2d array and has units of length\n",
    "   * note you have to do something about the zero mode, otherwise you divide by zero\n",
    "\n",
    "3) create a conditional based on $2\\pi/|k|$ (pick out the indices of the 2d array) and set the corresponding Fourier amplitudes to zero\n",
    "   * use `<` or `>` depending if you want to high/low pass\n",
    "   \n",
    "4) `ifft` of the modified 2d array of the Fourier amplitudes to get the result back\n",
    "\n",
    "The below code is a bit of a monster but essentially follows the aforementioned plan of attack.\n",
    "\n",
    "> NOTE: With the last step I found it necessary to add `np.real(np.ifft....)`, because there were some spurious imaginary values floating around (even though they are numerically zero). If you don't do that Python will complain, but might still plot.\n",
    "\n",
    "> <span style=\"color:red\">**Q.**</span> Go through the code below and convince yourself what I did is what I described above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2d filtering\n",
    "\n",
    "f_h = np.fft.fft(np.fft.fft(f, axis=0), axis=1)\n",
    "Nx, Ny = f_h.shape\n",
    "Lx = (df[\"lon\"][-1] - df[\"lon\"][0]).values * 100  # in units of km\n",
    "scale_factor_x = 2.0 * np.pi / Lx\n",
    "Ly = (df[\"lat\"][-1] - df[\"lat\"][0]).values * 100  # in units of km\n",
    "scale_factor_y = 2.0 * np.pi / Ly\n",
    "\n",
    "# creating the kx and ky by hand (+1 here to make sure the resulting corresponds to python FFT convention)\n",
    "kx_vec = np.concatenate([np.arange(Nx//2+1), np.arange(-Nx//2+1, 0)]) * scale_factor_x\n",
    "ky_vec = np.concatenate([np.arange(Ny//2+1), np.arange(-Ny//2+1, 0)]) * scale_factor_y\n",
    "\n",
    "# convert to array\n",
    "kx_mesh, ky_mesh = np.meshgrid(kx_vec, ky_vec)  # the k's here are already scaled to appropriate units of km\n",
    "k_array = np.sqrt(kx_mesh ** 2 + ky_mesh ** 2)\n",
    "k_array[k_array==0] = 1e-16  # avoid dividing by zero\n",
    "del kx_mesh, ky_mesh\n",
    "\n",
    "### original\n",
    "fig = plt.figure(figsize=(16, 9))\n",
    "ax = plt.subplot(2, 3, 1)\n",
    "df[\"speed\"].isel(time=0).plot(ax=ax)\n",
    "ax.grid()\n",
    "\n",
    "ax = plt.subplot(2, 3, 4)\n",
    "cs = ax.pcolormesh(kx_vec[:Nx//2], ky_vec[:Nx//2], np.log10(abs(f_h[:Nx//2, :Ny//2])**2), \n",
    "                   cmap=\"RdBu_r\", shading=\"auto\", vmin=-4, vmax=4)\n",
    "ax.set_xlabel(r\"$k_x$\")\n",
    "ax.set_ylabel(r\"$k_y$\")\n",
    "cb = plt.colorbar(cs)\n",
    "cb.set_ticks([-4, -2, 0, 2, 4])\n",
    "cb.set_ticklabels([r\"$10^{-4}$\", r\"$10^{-2}$\", r\"$10^0$\", r\"$10^2$\", r\"$10^4$\"])\n",
    "ax.set_title(r\"original spectrum\")\n",
    "ax.grid()\n",
    "\n",
    "### low pass\n",
    "ax = plt.subplot(2, 3, 2)\n",
    "\n",
    "# filter based on conditional (recall that 2pi / k is the wavelength here)\n",
    "km_thresh = 100\n",
    "f_h_mod = copy.deepcopy(f_h)\n",
    "f_h_mod[2.0*np.pi / k_array < km_thresh] = 0\n",
    "\n",
    "# ifft are done in reverse axis order to mirror the fft (doesn't actually matter)\n",
    "f_mod = np.real(np.fft.ifft(np.fft.ifft(f_h_mod, axis=1), axis=0)) # pick real part to mop up the small imag parts\n",
    "\n",
    "cs = ax.pcolormesh(df[\"lon\"], df[\"lat\"], f_mod, shading=\"auto\", vmin=0, vmax=f.max())\n",
    "cb = plt.colorbar(cs)\n",
    "ax.set_title(f\"low pass of field (removing < {km_thresh}km)\")\n",
    "ax.grid()\n",
    "\n",
    "ax = plt.subplot(2, 3, 5)\n",
    "cs = ax.pcolormesh(kx_vec[:Nx//2], ky_vec[:Nx//2], np.log10(abs(f_h_mod[:Nx//2, :Ny//2])**2), \n",
    "                   cmap=\"RdBu_r\", shading=\"auto\", vmin=-4, vmax=4)\n",
    "ax.set_xlabel(r\"$k_x$\")\n",
    "ax.set_ylabel(r\"$k_y$\")\n",
    "cb = plt.colorbar(cs)\n",
    "cb.set_ticks([-4, -2, 0, 2, 4])\n",
    "cb.set_ticklabels([r\"$10^{-4}$\", r\"$10^{-2}$\", r\"$10^0$\", r\"$10^2$\", r\"$10^4$\"])\n",
    "ax.set_title(r\"modified spectrum: low pass\")\n",
    "ax.grid()\n",
    "\n",
    "### high pass\n",
    "ax = plt.subplot(2, 3, 3)\n",
    "\n",
    "f_h_mod = copy.deepcopy(f_h)\n",
    "f_h_mod[2.0*np.pi / k_array > km_thresh] = 0\n",
    "\n",
    "# ifft are done in reverse axis order to mirror the fft (doesn't actually matter)\n",
    "f_mod = np.real(np.fft.ifft(np.fft.ifft(f_h_mod, axis=1), axis=0)) # pick real part to mop up the small imag parts\n",
    "\n",
    "cs = ax.pcolormesh(df[\"lon\"], df[\"lat\"], f_mod, shading=\"auto\", vmin=0, vmax=f.max()/4.0)\n",
    "cb = plt.colorbar(cs)\n",
    "ax.set_title(f\"high pass of field (removing > {km_thresh}km)\")\n",
    "ax.grid()\n",
    "\n",
    "ax = plt.subplot(2, 3, 6)\n",
    "cs = ax.pcolormesh(kx_vec[:Nx//2], ky_vec[:Nx//2], np.log10(abs(f_h_mod[:Nx//2, :Ny//2])**2), \n",
    "                   cmap=\"RdBu_r\", shading=\"auto\", vmin=-4, vmax=4)\n",
    "ax.set_xlabel(r\"$k_x$\")\n",
    "ax.set_ylabel(r\"$k_y$\")\n",
    "cb = plt.colorbar(cs)\n",
    "cb.set_ticks([-4, -2, 0, 2, 4])\n",
    "cb.set_ticklabels([r\"$10^{-4}$\", r\"$10^{-2}$\", r\"$10^0$\", r\"$10^2$\", r\"$10^4$\"])\n",
    "ax.set_title(r\"modified spectrum: high pass\")\n",
    "ax.grid()\n",
    "\n",
    "fig.tight_layout(pad=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As advertised, the low pass keeps only Fourier amplitudes within some circle in spectral space, and the resulting signal from the `ifft` is smudged over, removing some small-scale fluctuations. The corresponding high pass removes instead the large-scale variability and keeps the small-scale fluctuations; notice in this case the amplitude of the resulting signal is lower, as can be seen by the corresponding colour bar scales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting 2d to 1d spectrum: Isotropic assumption\n",
    "\n",
    "Lets suppose we actually believe there is no intrinsic preference in the direction, i.e. we assume the data is **isotropic** (in space). Then we only care about $k = |\\boldsymbol{k}| = \\sqrt{k_x^2 + k_y^2}$ rather than the individual components $k_x$ and $k_y$ separately. If we make this assumption (bypassing whether this is actually a valid assumption), then we can construct an analogous power spectrum based on $k = |\\boldsymbol{k}|$. For this, the plan of attack is as follows:\n",
    "\n",
    "1) given the re-scaled $k_x$ and $k_y$, work out the $|k|$\n",
    "\n",
    "2) flatten the resulting $|k|$ into a 1d array, and sort the array in ascending order of $|k|$\n",
    "   * I used the `np.argsort` command to pick out the appropriate indices, as `idx_sort` below\n",
    "   \n",
    "3) flatten the 2d array of Fourier amplitudes as well, and sort according to the index (in this case using `idx_sort`)\n",
    "\n",
    "4) compute power spectrum by taking absolute value and squaring\n",
    "\n",
    "The result is as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct 1d spectrum assuming isotropy\n",
    "f_h = np.fft.fft(np.fft.fft(f, axis=0), axis=1)\n",
    "Nx, Ny = f_h.shape\n",
    "\n",
    "# don't need the full spectrum so just do it with the 1st half\n",
    "kx_vec = np.arange(Nx//2+1) * scale_factor_x\n",
    "ky_vec = np.arange(Ny//2+1) * scale_factor_y\n",
    "\n",
    "kx_mesh, ky_mesh = np.meshgrid(kx_vec, ky_vec)  # the k's here are already scaled to appropriate units of km\n",
    "k_array = np.sqrt(kx_mesh ** 2 + ky_mesh ** 2)\n",
    "k_vec = k_array.flatten()\n",
    "\n",
    "del kx_mesh, ky_mesh, k_array\n",
    "\n",
    "idx_sort = np.argsort(k_vec)\n",
    "k_vec = k_vec[idx_sort]\n",
    "\n",
    "f2_vec = (np.abs(f_h[:Nx//2+1, :Ny//2+1])**2).flatten()\n",
    "f2_vec = f2_vec[idx_sort]\n",
    "\n",
    "# plots (k's have already been scaled accordingly)\n",
    "\n",
    "fig = plt.figure(figsize=(14, 3))\n",
    "ax = plt.subplot(1, 3, 1)\n",
    "ax.semilogx(k_vec[1::], f2_vec[1::], 'C0o-')\n",
    "ax.set_xlabel(r\"$k$ (rad per km)\")\n",
    "ax.set_ylabel(r\"$|\\hat{f}|^2$\")\n",
    "ax.set_title(r\"wavenumber\")\n",
    "ax.grid()\n",
    "\n",
    "ax = plt.subplot(1, 3, 2)\n",
    "ax.semilogx(k_vec[1::] / (2.0*np.pi), f2_vec[1::], 'C0o-')\n",
    "ax.set_xlabel(r\"$\\nu$ (km$^{-1}$)\")\n",
    "ax.set_title(r\"frequency\")\n",
    "ax.grid()\n",
    "\n",
    "ax = plt.subplot(1, 3, 3)\n",
    "ax.semilogx(2.0*np.pi / k_vec[1::], f2_vec[1::], 'C0o-')\n",
    "ax.set_xlabel(r\"$\\lambda$ (km)\")\n",
    "ax.set_title(r\"wavelength\")\n",
    "ax.grid()\n",
    "\n",
    "fig.tight_layout(pad=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The conclusions here are not hugely different from the 1d spatial power spectrum above. However the current spectrum uses all the data rather than just a subset like in the horizonal tracks abive, so might be argued to be more statistically representative.\n",
    "\n",
    "> <span style=\"color:red\">**Q.**</span> Try these at different times and see if the spectrum is robust. You may or may not want to zoom in to the power spectrum accordingly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------\n",
    "# c) Empirical Orthogonal Functions (EOFs)\n",
    "\n",
    "#### ***TL;DR: basically PCA***\n",
    "\n",
    "When we were dealing with Fourier series we essentially assumed we have a basis function in advance (see `08_time_series` and above) expand our function with the basis, and as a result we deal with the amplitude. This is all well and good if you know your basis in advance, but sometimes that doesn't work (e.g. complicated domain shapes), or you don't necessarily want to do that. We may for example seek something like\n",
    "\n",
    "\\begin{equation*}\n",
    "    f(t,x,y) = \\sum_{k=1}^N \\mbox{PC}_k(t)\\ \\mbox{EOF}_k(x,y)\n",
    "\\end{equation*}\n",
    "\n",
    "where the function $\\mbox{EOF}(x,y)$ is some spatial pattern that captures the most variability of the data in some sense, and tagged on with it some $\\mbox{PC}(t)$ that describes that pattern's evolution in time. The idea then is that the first few EOFs would be related to the large-scale persistent (in space and time) features, and other things would be additional details on top of that. We might then be more inclined to try and come up with theories that explain the dominant EOFs, or constrain theories based on whether they are able to reproduce these EOFs.\n",
    "\n",
    "Sounds familiar? Well that's basically what the PCA (`04_regression`) aims to do as well. The EOF analysis could be thought of as a generalised PCA, and while there are some detailed differences, the principles and the algorithms involved are more similar than they are different. \n",
    "\n",
    "The main technical difference is that, for the PCA, we essentially want to form the **covariance matrix** (see `04_regression`), which is necessarily symmetric by construction, diagnonalise the matrix, from which the eigenvectors are the PCs, and the eigenvalues are the variance explained. For the EOF, in general we start with $f(t,x,y)$, reshape it into $f(t, \\mathrm{space})$. Generically the array dimensions of $t$ and $\\mathrm{space}$ will not agree, and if we form the covariance matrix we would have to artificially bulk out locations with empty numbers to make sure the eventual covariance matrix is square. To avoid this, we generally consider what would be called a **singular value decomposition (SVD)**, which considers the decomposition\n",
    "\n",
    "\\begin{equation*}\n",
    "    \\mathbf{\\mathsf{F}} = \\mathbf{\\mathsf{U}} \\mathbf{\\mathsf{\\Sigma}} \\mathbf{\\mathsf{V}}^T,\n",
    "\\end{equation*}\n",
    "\n",
    "where:\n",
    "\n",
    "* $\\mathbf{\\mathsf{F}}$ is the matrix encoding $f(t, \\mathrm{space})$\n",
    "* $\\mathbf{\\mathsf{\\Sigma}}$ is the matrix containing the **singular values** $d\\sim \\sigma^2$, where $\\sigma^2$ is the variance\n",
    "* $\\mathbf{\\mathsf{U}}$ is the matrix of the **principal components**, where the *rows* correspond to $\\mbox{PC}_k(t)$ (e.g. `U[:, 0]` would be $\\mbox{PC}_1(t)$)\n",
    "* $\\mathbf{\\mathsf{V}}^T$ is the matrix containing the **empirical orthogonal functions**, where *columns* of $\\mathbf{\\mathsf{V}}^T$ correspond to $\\mbox{EOF}_k(\\mathrm{space})$ (e.g. `VT[0, :]` would be $\\mbox{EOF}_0(\\mathrm{space})$, which needs to be reshaped back to $\\mbox{EOF}_0(x, y)$)\n",
    "\n",
    "Put another way, the PCs and EOFs are really the **left/right singular vectors** ($\\mathbf{\\mathsf{U}}$ and $\\mathbf{\\mathsf{V}}$ are the matrices containing the singular vectors). The ordering is such that we are in descending order of the magnitude of the singular values. The maximum number of singular values (i.e. maximum size of $\\mathbf{\\mathsf{\\Sigma}}$) and thus the maximum number of singular vectors is the minimum array dimension of $t$ and $\\mathrm{space}$.\n",
    "\n",
    "> NOTE: Most PCA algorithms actually use the SVD approach, instead of the eigenvalue approach I described. SVDs can be applied to non-square matrices, and the algorithms are generally stable and fairly fast.\n",
    "\n",
    "We don't actually need to implement the EOF analysis by hand from scratch (unless you want to, it's not actually that hard; see optional exercise at the end). We can leverage `PCA` in the `scikit-learn` package as is, and the only thing we need to do is do some pre-processing of data before feeding data into the PCA, and do some post-processing afterwards for the EOFs (the PCs drops out as is)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: \"trivial\" sines and cosines\n",
    "\n",
    "Let's try it with a simple example first. The below code is copy and pasted from `09_fun_with_maps` for the purposes of demonstrating how to make animations. The function we define here is\n",
    "\n",
    "\\begin{equation*}\n",
    "    f(t, y, x) = \\sin(x)\\cos(y)\\sin(t) + 0.5 \\cos(2x) \\cos(2t).\n",
    "\\end{equation*}\n",
    "\n",
    "The first part on the right hand side is from `09_fun_with_maps` and spatially represented by circular blobs, and I added on the the second part, which are rolls varying in the $x$ direction only, for demonstration reasons. Given your now extensive experience with Fourier analysis (!), you would know that a Fourier analysis of $f(t,y,x)$ would unequivocally pick out the two contributions separately. The question here is what would the EOF analysis do? \n",
    "\n",
    "Intuitively, we would expect the EOF analysis to also pick out those components also. Given the choice of amplitudes I fed into defining $f$, we might expect that\n",
    "\n",
    "\\begin{equation*}\n",
    "    \\mbox{EOF}_1(x,y) \\sim \\sin(x)\\cos(y), \\qquad \\mbox{PC}_1(t) \\sim \\sin(t),\n",
    "\\end{equation*}\n",
    "\n",
    "\\begin{equation*}\n",
    "    \\mbox{EOF}_2(x,y) \\sim \\cos(2x), \\qquad \\mbox{PC}_2(t) \\sim \\cos(2t),\n",
    "\\end{equation*}\n",
    "\n",
    "\\begin{equation*}\n",
    "    \\mbox{EOF}_k(x,y) \\sim 0, \\qquad \\mbox{PC}_k(t) \\sim 0, \\qquad k \\geq 3,\n",
    "\\end{equation*}\n",
    "\n",
    "where I used the squiggle $\\sim$ to denote that these things would be defined up to a constant factor (negative or positive). Additionally, we should be able to reconstruct exactly (or at least numerically) the function $f$ from the EOFs and PCs.\n",
    "\n",
    "Let's define the raw data first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# snapshots with a different colour map to show oscillations\n",
    "\n",
    "x_vec = np.linspace(0, 2*np.pi, 21)\n",
    "y_vec = np.linspace(0, 2*np.pi, 26)\n",
    "t_vec = np.linspace(0, 2*np.pi, 21)\n",
    "\n",
    "xx, yy = np.meshgrid(x_vec, y_vec)  # creates a mesh (a 2d array) from the 1d arrays\n",
    "f = np.zeros((len(t_vec), len(y_vec), len(x_vec)))  # use (t, y, x) ordering\n",
    "for kt in range(len(t_vec)):\n",
    "    t = t_vec[kt]\n",
    "    f[kt, :, :] =  (        np.sin(  xx) * np.cos(yy) * np.sin(t)\n",
    "                    + 0.5 * np.cos(2*xx)              * np.cos(2*t)\n",
    "                   )\n",
    "del xx, yy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that from the PCA we want to scale the data and get some anomalies (done by a $Z$-transform, through the `StandardScaler` in `sklearn`). For the EOF we want to scale the input data too, but it looks like in this case we want to de-trend or at least de-mean in time per spatial point, rather than do a $Z$-transform.\n",
    "\n",
    "The below carries out a de-trending (linear trend) using `scipy.signal` only over the time dimension; the data here is arranged as $(t,y,x)$ so it is `axis=0` that we want.\n",
    "\n",
    "> NOTE: If we de-mean, then you want to add the mean back on right at the end when (and if) you are trying to reconstruct the original function from the EOFs and PCs. If we de-trend, then it looks like you basically don't need to do anything more. I can't get it working for the $Z$-transform at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# probably want to at least de-mean in time \n",
    "#   if de-mean, take mean off, get EOFs, but add mean back on to final result)\n",
    "# f_mean = np.mean(f, axis=0)\n",
    "\n",
    "# detrend (to get anomalies with respect to linear trend in time) seems to work fine\n",
    "#    with some minor points in the PCs\n",
    "f = signal.detrend(f, axis=0)  # could also use keyword \"overwrite_data=True\"\n",
    "\n",
    "# Z-transform (from StandardScaler) seems to screw up the EOFs, so not doing that here (happy to be corrected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next bit is to reshape the matrix from $f(t, y, x)$ into $f(t, \\mathrm{space})$, so from a 3d to a 2d array. This is done via the `np.reshape` command. We will be passing the 2d array to the PCA in `scikit-learn`, and at some point we will want to invert the (2d) EOFs back into 3d.\n",
    "\n",
    "> NOTE: If you look these things up online you might see that some of the Python implementations have the optional keyword `order=\"F\"`, which means Fortran ordering (which differs from the default which is `order=\"C\"`, C ordering). I have not found that it made any difference as long as you are consistent when reshaping and undoing the reshaping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape from (t, y, x) into (t, space), \n",
    "# NOTE: order='F' being Fortran ordering might be seen sometimes\n",
    "#       don't think this matters as long as you are consistent in the reshaping\n",
    "\n",
    "X = np.reshape(f, (len(t_vec), len(y_vec) * len(x_vec)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we pass the 2d array to the PCA routine, which we already encountered in `04_regression`, so I am just going to state it. The PCs in this case is the transformation of the input data, while the EOFs are the components. Below I print out the explained variance ratio (it is a variance presumably converted from the calculated singular values), as well reshape the $\\mbox{EOF}_k(\\mathrm{space})$, which is a 2d array (labelled here by $k$ and $\\mathrm{space}$) back into $\\mbox{EOF}_k(y, x)$, which is a 3d array (labelled here by $k$, $y$ and $x$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basically do a PCA\n",
    "pca = PCA(n_components=5)\n",
    "\n",
    "# pull out the PCs and EOFs\n",
    "PCs             = pca.fit_transform(X)\n",
    "EOFs_to_reshape = pca.components_\n",
    "\n",
    "print(f\"pca var explained = {pca.explained_variance_ratio_ * 100} %\")\n",
    "\n",
    "# reshape the EOFs from (EOF number, space) to  (EOF number, y, x)\n",
    "EOFs = np.reshape(EOFs_to_reshape, (EOFs_to_reshape.shape[0], len(y_vec), len(x_vec)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the variance explained, we can see that basically all the variance is captured by the first two EOFs, which is what we would expect. We plot out the first two EOFs (which is a function of space) and their associated PCs (which is a function of time) below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12, 8))\n",
    "ax = plt.subplot(2, 2, 1)\n",
    "cs = ax.contourf(x_vec, y_vec, EOFs[0, :, :], cmap=\"RdBu_r\")\n",
    "ax.set_title(r\"$\\mathrm{EOF}_1(x,y)$\")\n",
    "ax.set_ylabel(r\"$y$\")\n",
    "plt.colorbar(cs)\n",
    "\n",
    "ax = plt.subplot(2, 2, 2)\n",
    "ax.plot(t_vec, PCs[:, 0])\n",
    "ax.set_title(r\"$\\mathrm{PC}_1(t)$\")\n",
    "ax.grid()\n",
    "\n",
    "ax = plt.subplot(2, 2, 3)\n",
    "cs = ax.contourf(x_vec, y_vec, EOFs[1, :, :], cmap=\"RdBu_r\")\n",
    "ax.set_title(r\"$\\mathrm{EOF}_2(x,y)$\")\n",
    "ax.set_xlabel(r\"$x$\")\n",
    "ax.set_ylabel(r\"$y$\")\n",
    "plt.colorbar(cs)\n",
    "\n",
    "ax = plt.subplot(2, 2, 4)\n",
    "plt.plot(t_vec, PCs[:, 1])\n",
    "ax.set_xlabel(r\"$t$\")\n",
    "ax.set_title(r\"$\\mathrm{PC}_2(t)$\")\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, interestingly, while the PCs corresponding to the EOFs are what we expected (e.g. the $\\cos(2x)$ patterns with the $\\cos(2t)$ time series), the EOF ordering is the other way round. It turns out this is somewhat of an artifact of the choice of pre-processing: if you do a de-mean procedure instead then the EOFs are the \"right\" way round. Notice here PC2 looks a bit weird, in that it's not exactly a $\\sin(t)$, which is what we imposed when defining $f(t, y, x)$.\n",
    "\n",
    "In that regard, this is a warning that the EOF analysis requires some sort of anomalies, but can be sensitive to the choice of pre-processing.\n",
    "\n",
    "The below test utilises the EOFs and PCs to reconstruct the data via\n",
    "\n",
    "\\begin{equation*}\n",
    "    f(t,x,y) = \\sum_{k=1}^2 \\mbox{PC}_k(t)\\ \\mbox{EOF}_k(x,y).\n",
    "\\end{equation*}\n",
    "\n",
    "The plot is per time $\\hat{t}$, and shows the two $\\mbox{PC}_k(\\hat{t})\\ \\mbox{EOF}_k(x,y)$, the full data $f(\\hat{t}, y, x)$, and the signed mismatch between $f(\\hat{t}, y, x)$ and $\\sum_{k=1}^2 \\mbox{PC}_k(\\hat{t})\\ \\mbox{EOF}_k(x,y)$. The signed mismatch can be shown to be tiny and essentially at the machine level for all the time levels.\n",
    "\n",
    "> <span style=\"color:red\">**Q.**</span> Don't take my word for it, try it and convince yourself I didn't just outright lie in the above two claims (the dependence on pre-processing and the smallness of the signed mismatch).\n",
    "\n",
    "> <span style=\"color:red\">**Q.**</span> Ideally by code rather than examining the plots visually, demonstrate that numerically the mismatches are small over time (hint: consider examining the absolute value of the mismatch at every single time step)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reconstruct the data at some time\n",
    "\n",
    "t_ind = 19  # field constructued here to not be identically zero at any choice of t\n",
    "\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "\n",
    "ax = plt.subplot(2, 2, 1)\n",
    "cs = ax.contourf(x_vec, y_vec, EOFs[0, :, :] * PCs[t_ind, 0], cmap=\"RdBu_r\")\n",
    "ax.set_ylabel(r\"$y$\")\n",
    "ax.set_title(f\"$\\mathrm{{EOF}}_1(x,y) \\mathrm{{PC}}_1(t={t_vec[t_ind]/(2.0*np.pi):.2f}$ wavelength$)$\")\n",
    "plt.colorbar(cs)\n",
    "\n",
    "ax = plt.subplot(2, 2, 2)\n",
    "cs = ax.contourf(x_vec, y_vec, EOFs[1, :, :] * PCs[t_ind, 1], cmap=\"RdBu_r\")\n",
    "ax.set_title(f\"$\\mathrm{{EOF}}_2(x,y) \\mathrm{{PC}}_2(t={t_vec[t_ind]/(2.0*np.pi):.2f}$ wavelength$)$\")\n",
    "plt.colorbar(cs)\n",
    "\n",
    "ax = plt.subplot(2, 2, 3)\n",
    "cs = ax.contourf(x_vec, y_vec, f[t_ind, :, :], cmap=\"RdBu_r\")\n",
    "ax.set_title(f\"$f(x,y, t={t_vec[t_ind]/(2.0*np.pi):.2f}$ wavelength$)$\")\n",
    "ax.set_xlabel(r\"$x$\")\n",
    "ax.set_ylabel(r\"$y$\")\n",
    "plt.colorbar(cs)\n",
    "\n",
    "# reconstruct the data from the EOFs\n",
    "f_EOFs = np.zeros((len(y_vec), len(x_vec)))\n",
    "for i in range(2):\n",
    "    f_EOFs += EOFs[i, :, :] * PCs[t_ind, i]\n",
    "\n",
    "ax = plt.subplot(2, 2, 4)\n",
    "# cs = ax.contourf(x_vec, y_vec, f[t_ind, :, :] - (f_EOFs+f_mean))  # put the mean back in\n",
    "cs = ax.contourf(x_vec, y_vec, f[t_ind, :, :] - f_EOFs, cmap=\"RdBu_r\")  # detrend case\n",
    "ax.set_title(r\"$f - f_{\\rm EOFs}$\")\n",
    "ax.set_xlabel(r\"$x$\")\n",
    "plt.colorbar(cs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: ERA SST reanalysis data\n",
    "\n",
    "Lets try this for real ocean data, partly to demonstrate extra things that we might need to be careful with the analysis, in this case when we encounter land points.\n",
    "\n",
    "Below code uses xarray to load the monthly [extended reconstructed SST anomaly data](https://www.eea.europa.eu/data-and-maps/data/external/noaa-extended-reconstructed-sea-surface), and just prints out the meta data.\n",
    "\n",
    "> NOTE: We are going to deal directly with the provides anomalies `ssta`. There is the full `sst` data on the repository too, but I have to say I have not managed to reconstruct the anomalies from the full data. This is partly because I am not entirely sure what the anomalies are relative to (it says relative to some climatology, but that didn't seem to work for me)...\n",
    ">\n",
    "> If I were to redo this I would actually get rid of the linear trend in time from the full data to remove the global warming signal. You can try this yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget https://github.com/julianmak/OCES3301_data_analysis/raw/main/ersstv5_ssta.nc  # processed anomaly data\n",
    "# !wget https://github.com/julianmak/OCES3301_data_analysis/raw/main/ersstv5_sst.nc   # full data\n",
    "\n",
    "# load the already processed anomalies\n",
    "#   (I have not managed to reconstruct the anomalies from the \"raw\" sst data, going to live with this for now)\n",
    "# df = xr.open_dataset(\"ersstv5_sst.nc\")\n",
    "\n",
    "df = xr.open_dataset(\"ersstv5_ssta.nc\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am going to analyse the Atlantic (the Pacific would be for the assignment or as an extended exercise later). This is partly to demonstrate another issue: data arrangement. \n",
    "\n",
    "Note that from the meta data, the longitude goes from 0 to 360. While the data technically is to be interpreted as periodic in longitude, because of how it is set up the data is centered over the Pacific. If we want the Atlantic (in this case between -80 and 10), xarray would complain in the selection, either giving you longitudes from 0 to 10, or from 280 to 360 (which is -80 to 0). A simple fix leveraging xarray capabilities is to just shift the co-ordinate, which is done with the `assign_coords` for an xarray dataset (the `% 360` is *modulo 360*, where the input numbers gets a multiple of 360 taken off to put the input numbers between 0 and 359). We are then going to do the selection using the xarray function `sel` (saves us having to look up the corresponding indices).\n",
    "\n",
    "From the meta data, we also see that there is a hanging `lev` dimension floating around (it's just of length 1), so getting rid of that by doing `isel(lev=0)`. Going to follow the zeroth rule of data analysis and just plot a sample of the data to see what it looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selection over Atlantic\n",
    "# current longitude is (0, 360), focus over Pacific\n",
    "# want Atlantic, quickest thing is to shift the longitude (-180, 180) so centered over the Atlantic\n",
    "df = df.assign_coords(lon=(((df[\"lon\"] + 180) % 360) - 180)).sortby('lon')\n",
    "\n",
    "target_lon, target_lat = slice(-80, 10), slice(0, 90)\n",
    "\n",
    "# trivial selection to drop the z co-ordinate (there is only 1)\n",
    "ssta = df[\"ssta\"].isel(lev=0).sel(lon=target_lon, lat=target_lat)\n",
    "\n",
    "# pull out other useful things to carry around\n",
    "time = df[\"time\"].values\n",
    "lon  = df[\"lon\"].sel(lon=target_lon).values\n",
    "lat  = df[\"lat\"].sel(lat=target_lat).values\n",
    "ssta.isel(time=0).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the white entries in the plot are the **land points**, which in the array is indicated by a NaN (you can look this up yourself by examining the data matrix itself). What you will find is that PCA refuses  to handle NaN entries, so we have to do something about it. There are two ways to do this:\n",
    "\n",
    "1) use masked arrays in via `numpy.ma` probably\n",
    "\n",
    "2) only do the PCA over the ocean points (I am going to call these **wet points**)\n",
    "\n",
    "I am going to do the second procedure as it is more robust and general (although somewhat more involved); try the first one as a coding excercise. \n",
    "\n",
    "First I am going to reshape the array into $(t, \\mathrm{space})$ as above. Then, since we know the land points correspond to NaN entries, we pick out the indices that are *not* NaNs, done through:\n",
    "\n",
    "* `np.where(CONDITION)` which picks out the indices corresponding the conditions\n",
    "  * `np.where` returns a tuple containing the indices, so there is a `[0]` to pick out the indices\n",
    "* `np.isnan(ARRAY)` to identify NaN entries with `True`\n",
    "  * `~` is `NOT`, so `~np.isnan(ARRAY)` is to pick out the entries that are *not* NaNs\n",
    "  * we assume the land points don't change in time, so we only need to query the 1st time entry, hence `ARRAY[0, :]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# already have anomalies, so could just go straight ahead with the EOF analysis\n",
    "#   not going to weight the gridcells here by the area\n",
    "\n",
    "X = np.reshape(ssta.values, (len(time), len(lat) * len(lon)))\n",
    "wet_pts = np.where(~np.isnan(X[0, :]))[0]  # pick out indices corresponding to ocean pts at first time index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now throw this into the PCA algorithm, keeping quite a few components. The below output shows a bar graph indicating the percentage of variance explained per EOF (as the blue bars), and the *cumulative* percentage of variance explained by the first $k$ number of EOFs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basically do a PCA:\n",
    "pca = PCA(n_components=20)\n",
    "\n",
    "# pull out the PCs and EOFs\n",
    "PCs             = pca.fit_transform(X[:, wet_pts])  # only do EOF analysis over the wet points\n",
    "EOFs_to_reshape = pca.components_\n",
    "\n",
    "print(f\"pca var explained = {pca.explained_variance_ratio_}\")\n",
    "\n",
    "fig = plt.figure(figsize=(6, 4))\n",
    "ax = plt.axes()\n",
    "ax.bar(np.arange(0, 20), pca.explained_variance_ratio_, color=\"C0\",\n",
    "       label=r\"EOF var explained\",\n",
    "       zorder=3)\n",
    "ax.bar(np.arange(0, 20), np.cumsum(pca.explained_variance_ratio_), color=\"C1\",\n",
    "       label=\"total EOF explained\",\n",
    "       zorder=1, alpha=0.7)  # push to the back\n",
    "ax.legend()\n",
    "ax.grid(lw=0.5, zorder=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice here unlike the artifical example above, the EOFs individually don't explain that much variance, so we might want to keep a few more if we wanted to reconstruct the original signal (I didn't bother, but you can do this as an exercise).\n",
    "\n",
    "The code below then reshapes the EOFs back to real space, but here we need to deal with the land points. So what I do below is:\n",
    "\n",
    "* set up a ($\\mathrm{EOF}, \\mathrm{space}$) array where everything are NaNs\n",
    "* with that big NaN array, cycle through the EOF dimension, and overwrite the corresponding wet points according to the `wet_pts` indices with the EOF entries\n",
    "* once all the EOFs have been dumped out, reshape the EOFs to ($\\mathrm{EOF}, y, x$) array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape the EOFs from (EOF number, space) to  (EOF number, y, x)\n",
    "\n",
    "# set a load of land points as (EOF, space)\n",
    "EOFs = np.ones((EOFs_to_reshape.shape[0], len(lat) * len(lon))) * np.nan\n",
    "\n",
    "# overwrite the places where it is actually ocean, per EOF\n",
    "for k in range(EOFs.shape[0]):\n",
    "    EOFs[k, wet_pts] = EOFs_to_reshape[k, :]\n",
    "    \n",
    "# reshape the EOFs back to (EOF, lat, lon)\n",
    "EOFs = np.reshape(EOFs, (EOFs.shape[0], len(lat), len(lon)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the below I plot out the first four EOFs and the corresponding PCs. Interpretation in the cell after."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot a few EOFs out\n",
    "pcarree = ccrs.PlateCarree()\n",
    "fig = plt.figure(figsize=(16, 6))\n",
    "for k in range(4):\n",
    "    \n",
    "    # pull out the plots and set the \n",
    "    EOF = EOFs[k, :, :]\n",
    "    limit = np.nanmax(np.nanmax(np.abs(EOF)))\n",
    "    levels = np.linspace(-limit, limit, 16)\n",
    "    \n",
    "    ax = plt.subplot(2, 4, 2*k+1, projection=pcarree)\n",
    "    cs = ax.contourf(lon, lat, EOF, cmap=\"RdBu_r\", levels=levels)\n",
    "    gl = ax.gridlines(crs=pcarree, draw_labels=True,\n",
    "                      linewidth=2, color='gray', alpha=0.5, linestyle='--')\n",
    "    gl.top_labels = False\n",
    "    gl.right_labels = False\n",
    "    ax.add_feature(cartopy.feature.LAND, zorder = 10, edgecolor = 'k')\n",
    "    ax.set_title(f\"$\\mathrm{{EOF}}_{k+1}$\")\n",
    "    \n",
    "    divider = make_axes_locatable(ax)  # add a colorbar\n",
    "    cb_ax = divider.append_axes(\"right\", size = \"2%\", pad = 0.2, axes_class=plt.Axes)\n",
    "    cax = plt.colorbar(cs, cax=cb_ax)\n",
    "    cax.set_ticks([-limit, 0, limit])\n",
    "    \n",
    "    ax = plt.subplot(2, 4, 2*k+2)\n",
    "    ax.plot_date(time, PCs[:, k], color=\"C0\", fmt=\"\")\n",
    "    ax.set_xlabel(f\"$t$\")\n",
    "    ax.set_ylabel(f\"$\\mathrm{{PC}}_{k+1}$\")\n",
    "    plt.setp(plt.gca().xaxis.get_majorticklabels(),\n",
    "        'rotation', 90)\n",
    "    ax.grid()\n",
    "\n",
    "fig.tight_layout(pad=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With regards to interpretations, I am going to speculate a bit here, and I am willing to be shown wrong in my interpretations.\n",
    "\n",
    "### EOF 1\n",
    "\n",
    "I would have initially thought this would correspond to the global warming signal (partly given that the PCs are increasing over time), since I am almost certain that the anomalies are not relative to the linear trend over the whole period so that the global warming signal has not been removed. It probably also includes hints of the [**Atlantic Multi-decadal Variability (AMV)**](https://en.wikipedia.org/wiki/Atlantic_multidecadal_oscillation), which is a long-time scale variability on the order of 40 or so years probably (a quick look at the power spectrum suggests there is a peak at 60 years).\n",
    "\n",
    "### EOF 2\n",
    "\n",
    "This one is probably related to the [**North Atlantic Oscillation (NAO)**](https://en.wikipedia.org/wiki/North_Atlantic_oscillation), with the sandwich pattern with the filling just south of Greenland, sandwiched between the blobs Gulf Stream region and North East of Iceland (very technical language here). The NAO signal is more clearly seen by the leading EOF of the sea level pressure over the North Atlantic, but also has a signal in the SST as above.\n",
    "\n",
    "### EOF 3\n",
    "\n",
    "Apart from this looking like the EOF 2 in Figure 3 of [Buckley et al. (2014)](https://www.researchgate.net/publication/277677765_Low-Frequency_SST_and_Upper-Ocean_Heat_Content_Variability_in_the_North_Atlantic/figures?lo=1), I don't have much more to add...\n",
    "\n",
    "### EOF 4\n",
    "\n",
    "No idea. This one seems to have a long period, so may also be related somewhat to AMV."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> NOTE: This stuff is probably also highlighting that there are a few things one might need to do as pre/post-processing to get more information. These would include:\n",
    ">\n",
    "> 1) taking the global warming trend out, which would modify the EOF analysis probably (pre-processing)\n",
    ">\n",
    "> 2) might want to also take the seasonal cycle out via averaging over a few months (pre/post-processing)\n",
    ">\n",
    "> 3) window the PCs a bit to make the time-series analysis a bit clearer (post-processing)\n",
    "\n",
    "> NOTE: I am personally not a big fan of EOFs, since not everything is a mode, not everything is linear as implied by the EOF analysis, and just because one identifies an EOF does not by itself provide anything about the underlying mechanisms (correlation does not necessarily imply causation).\n",
    "\n",
    "> <span style=\"color:red\">**Q.**</span> Do a time-series analysis on the PCs, picking out periods and the like.\n",
    "\n",
    "> <span style=\"color:red\">**Q.**</span> Try doing some of the things I suggested above.\n",
    "\n",
    "> <span style=\"color:red\">**Q.**</span> (coding) I am sure there are better ways to write the code that I did above, particularly with making better use of the xarray functionalities. Please do improve on my code (let me know if you do if you are happy to share).\n",
    "\n",
    "> <span style=\"color:red\">**Q.**</span> (coding) Try replacing my indexing procedure to get wet points with using the masked array functionality.\n",
    "\n",
    "> <span style=\"color:red\">**Q.**</span> (exploratory) Try different spatial regions. The equatorial Pacific is a particularly interesting region."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------\n",
    "# More involved exercises with this notebook\n",
    "\n",
    "## 1) Spectral analysis for GEBCO bathymetry data\n",
    "\n",
    "Consider trying the Fourier analyses things on GEBCO data. Some things to bear in mind and think about:\n",
    "\n",
    "1) why might it not be valid to do a Fourier analysis with the global GEBCO data?\n",
    "\n",
    "2) if you do an analysis over a large region, you may want to adjust your conversion from lon/lat degrees to (kilo)meters from the one I used, because the assumption of 1 degree = 100 km then is even more dodgy, since the conversion is a function of latitude\n",
    "\n",
    "3) consider doing the analyses in patches, and see if there is some robustness in the power spectrum (e.g. [Goff & Jordan, 1989](https://agupubs.onlinelibrary.wiley.com/doi/epdf/10.1029/GL016i001p00045))\n",
    "\n",
    "<img src=\"https://i.imgur.com/bvclGhT.jpg\" width=\"600\" alt='picture of patches'>\n",
    "\n",
    "(a picture of the (in)famous Patches from the From Software games)\n",
    "\n",
    "4) consider doing the ocean and land parts separately\n",
    "\n",
    "5) [pretty involved] the proper thing to do is probably something like a **spherical harmonic analysis**, but you will probably need to go beyond native `numpy` and `scipy` packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) [Probably quite involved] Spilhaus projection\n",
    "\n",
    "Work out how you might make the [Spilhaus projection](https://storymaps.arcgis.com/stories/756bcae18d304a1eac140f19f4d5cb3d) in Python:\n",
    "\n",
    "<img src=\"https://www.researchgate.net/profile/Michael-Meredith-5/publication/334126444/figure/fig2/AS:775590957309952@1561926702371/The-globe-viewed-on-a-Spilhaus-projection-in-contrast-to-conventional-projections-this_W640.jpg\" width=\"400\" alt='spilhaus'>\n",
    "\n",
    "(Cursed projection? Figure from Mike Meredith.)\n",
    "\n",
    "There is some work being done on the Cartopy repository itself with sporadic activity. If you get this working, I would suggest you make a pull request for Cartopy, because that would certainly be an addition I'd like to see in Cartopy (I think the above is made is a GIS software)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## c) Analysis of `current_speed.nc`\n",
    "\n",
    "Try the EOF analysis for the `current_speed.nc` data and see if anything interesting comes out. There should be no land points. Try also doing some time series analysis on the resulting PCs (e.g. Fourier analysis, suitably averaged etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## d) SVDs by hand\n",
    "\n",
    "Try doing EOFs by hand through the SVD command `np.linalg.svd(X, full_matrices=False)` (the last optional keyword means you don't save the singular values as a diagonal matrix).\n",
    "\n",
    "You can also try using SVDs for time-series analysis or PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
