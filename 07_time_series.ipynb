{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# JM: 23 Dec 2021\n",
    "# notebook to go through \"basic\" python and notebook things\n",
    "\n",
    "# mantra of the course: If you have a code problem, try Google first. A large part of programing is\n",
    "#                       experience, and you gain experience more efficiently by trying to fix code yourself.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time-series data\n",
    "# by time-series data we just mean some data that depends on time\n",
    "# an example here is the El Nino 3.4 data given in 02_reading_data_basic_manipulations\n",
    "\n",
    "# the code below reads the data in the old-fashioned way to get the data as a 1d array\n",
    "# Q. try do this in pandas instead\n",
    "\n",
    "with open(\"elnino34_sst.data\", \"r\") as f:\n",
    "    elnino34_txt = f.readlines()\n",
    "elnino34_txt = elnino34_txt[3:-4]  # strip out some unnecessary lines\n",
    "for k in range(len(elnino34_txt)):\n",
    "    elnino34_txt[k] = elnino34_txt[k].strip(\"\\n\")\n",
    "\n",
    "# then we split each line (as a string) up into components\n",
    "elnino34_txt[0].split()\n",
    "\n",
    "# so we could define an empty list, cycle through each line, split, and add in the entries\n",
    "# but skipping the first one if we only want the SST entries\n",
    "\n",
    "elnino34_sst = []\n",
    "for k in range(len(elnino34_txt)):           # this is the new elnino34_txt after stripping out some lines\n",
    "    dummy = elnino34_txt[k].split()          # split out the entries per line\n",
    "    for i in range(1, len(dummy)):           # cycle through the dummy list but skip the first entry\n",
    "        elnino34_sst.append(float(dummy[i])) # turn string into a float, then add to list\n",
    "\n",
    "elnino34_sst = np.array(elnino34_sst)\n",
    "\n",
    "plt.plot(elnino34_sst)\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# but we do not have a time to plot it against\n",
    "# so what you would see by reading the raw data itself is that this is monthly SST data from 1950 to 2019\n",
    "# so in this case one can create a time array to plot the data against\n",
    "# you could either 1) do this as an artifical raw array (you have to deal with units yourself)\n",
    "#                  2) use the datetime64 object functionality\n",
    "# below are code to do both\n",
    "\n",
    "# 1) ad hoc: 1 Jan 1950 to 31 Dec 2019\n",
    "#            use linspace, use length of the read in array, but leave out the last point\n",
    "#            so \"1950.00\" correpsonds to the 1 Jan 1950\n",
    "#               \"2019.95\" or something like that corresponds to 31 Dec 2019\n",
    "#\n",
    "# the result is an array that you can manipulate as usual\n",
    "\n",
    "time_vec_raw = np.linspace(1950, 2019+1, len(elnino34_sst), endpoint=False)\n",
    "\n",
    "# 2) datetime: this one is easier to read, but slightly less trivial to manipulate \n",
    "#              (which matters somewhat in 08_time_series)\n",
    "#              in this case you don't specify the DATE and smaller units (syntax reasons)\n",
    "#\n",
    "# the end result is an array but instead of numbers it is data in the \"datetime64\" format\n",
    "# syntax of \"arange\" is np.arange(start, end but not including, spacing), so need to the \"end\" slightly larger\n",
    "time_vec = np.arange(np.datetime64('1950-01'), np.datetime64('2020-01'), np.timedelta64(1, 'M'))\n",
    "\n",
    "fig = plt.figure(figsize=(10, 7))\n",
    "ax = plt.subplot(2, 1, 1)\n",
    "ax.plot(time_vec_raw, elnino34_sst)\n",
    "ax.set_ylabel(r\"SST (${}^\\circ\\mathrm{C}$)\")\n",
    "ax.grid()\n",
    "\n",
    "ax = plt.subplot(2, 1, 2)\n",
    "ax.plot(time_vec, elnino34_sst)\n",
    "ax.set_xlabel(r\"$t$ (years)\")\n",
    "ax.set_ylabel(r\"SST (${}^\\circ\\mathrm{C}$)\")\n",
    "ax.grid()\n",
    "\n",
    "# if you wanted to create things in smaller units (e.g. days), you might do\n",
    "# time_vec = np.arange(np.datetime64('1950-01-01'), np.datetime64('1950-12-31'), np.timedelta64(1, 'd'))\n",
    "# look up online the relevant syntax\n",
    "\n",
    "# we will come back to the El-Nino data later in the exercises and in 08_time_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some time-series manipulations\n",
    "\n",
    "# create and use an artificial one for now for making a point\n",
    "\n",
    "time_vec = np.arange(np.datetime64('2020-01-01'), np.datetime64('2021-12-31'), np.timedelta64(6, 'h'))\n",
    "\n",
    "nt = len(time_vec)\n",
    "t_vec = np.linspace(0, 2.0 * np.pi, nt)\n",
    "lin_trend = 0.05 * np.linspace(0, 2.0 * np.pi, nt)\n",
    "\n",
    "noise = 0.2 * np.random.rand(nt)\n",
    "f_vec = (  2.7 \n",
    "         + 0.1 * np.sin(t_vec) \n",
    "         + 0.05 * np.sin(4.0 * t_vec) \n",
    "         + 0.02 * np.sin(60.0 * t_vec) \n",
    "         + lin_trend \n",
    "         + noise\n",
    "        )\n",
    "\n",
    "fig = plt.figure(figsize=(10, 3))\n",
    "ax = plt.axes()\n",
    "ax.plot(time_vec, f_vec, 'C0-')\n",
    "ax.set_xlabel(r\"$t$\")\n",
    "ax.set_ylabel(r\"data\")\n",
    "ax.set_ylim([2.7, 3.2])\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# too much data, there looks like a trend, but how to pick it out?\n",
    "# one way is to just brute force downsize\n",
    "\n",
    "fig = plt.figure(figsize=(10, 3))\n",
    "ax = plt.axes()\n",
    "ax.plot(time_vec[::20], f_vec[::20], 'C0-')  # Q: what does this do?\n",
    "ax.set_xlabel(r\"$t$\")\n",
    "ax.set_ylabel(r\"data\")\n",
    "ax.set_ylim([2.7, 3.2])\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# but the above is losing way too much information, and also not really filtering out signals\n",
    "# consider averaging data over a window (uniform weighting)\n",
    "#    (need to chop off some entries at the edges to have the same array length to plot)\n",
    "\n",
    "window = 60                                             # specify a window (number of entries)\n",
    "f_vec_uni_avg = np.zeros(len(f_vec[window:-window:]))   # final array with edges chopped off\n",
    "for i in range(len(f_vec[window:-window:])):\n",
    "    f_vec_uni_avg[i] = np.mean(f_vec[i:i+window:])      # uniform average over a window\n",
    "    \n",
    "fig = plt.figure(figsize=(10, 3))\n",
    "ax = plt.axes()\n",
    "ax.plot(time_vec[window:-window:], f_vec_uni_avg, 'C0-')  # Q: what does this do?\n",
    "ax.set_xlabel(r\"$t$\")\n",
    "ax.set_ylabel(r\"data\")\n",
    "ax.set_ylim([2.7, 3.2])\n",
    "ax.grid()\n",
    "\n",
    "# notice this averages out the fast fluctuations without down-sampling as such \n",
    "# but also the values have decreased in magnitude (because of the averaging)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# consider averaging data over a window (tent-like weighting, purely artificial)\n",
    "#    (need to chop off some entries at the edges to have the same array length to plot)\n",
    "\n",
    "window = 60                                                        # specify a window (number of entries)\n",
    "f_vec_tent_avg = np.zeros(len(f_vec[window:-window:]))             # final array with edges chopped off\n",
    "\n",
    "tent_kernel  = -np.abs(np.arange(window)+0.5 - window/2.0)         # make a straight line and bend it into a v\n",
    "tent_kernel -= np.min(tent_kernel)                                 # flip the v upside down\n",
    "tent_kernel /= np.max(tent_kernel) / 2.0                           # normalise such that kernel itself sums to 1\n",
    "\n",
    "for i in range(len(f_vec[window:-window:])):\n",
    "    f_vec_tent_avg[i] = np.mean(tent_kernel * f_vec[i:i+window:])  # average over a window with weighting\n",
    "    \n",
    "fig = plt.figure(figsize=(14, 3))\n",
    "ax = plt.subplot2grid((1, 3), (0, 0), colspan=2)\n",
    "ax.plot(time_vec[window:-window:], f_vec_tent_avg, 'C0-')         # Q: what does this do?\n",
    "ax.set_xlabel(r\"$t$\")\n",
    "ax.set_ylabel(r\"data\")\n",
    "ax.set_ylim([2.7, 3.2])\n",
    "ax.grid()\n",
    "\n",
    "ax = plt.subplot2grid((1, 3), (0, 2), colspan=1)\n",
    "ax.plot(tent_kernel, 'C1-')  # Q: what does this do?\n",
    "ax.set_xlabel(r\"index\")\n",
    "ax.set_ylabel(r\"kernel shape\")\n",
    "ax.set_ylim([0.0, 2.0])\n",
    "ax.grid()\n",
    "\n",
    "# fairly minor differences in this case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# consider averaging data over a window (Gaussian kernel with deconvolution)\n",
    "#    could write this out raw but just going to call a pacakge here...\n",
    "\n",
    "from scipy.ndimage import filters\n",
    "\n",
    "# here the value to specify is sigma (what it means somewhat determines on the kernel shape)\n",
    "\n",
    "sigma = 2.0\n",
    "f_vec_gauss_avg = filters.gaussian_filter1d(f_vec, sigma)\n",
    "\n",
    "fig = plt.figure(figsize=(10, 7))\n",
    "ax = plt.subplot(2, 1, 1)\n",
    "ax.plot(time_vec, f_vec_gauss_avg, 'C0-', label=r\"$\\sigma = %.1f$\" % sigma)\n",
    "ax.set_xlabel(r\"$t$\")\n",
    "ax.set_ylabel(r\"data\")\n",
    "ax.set_ylim([2.7, 3.2])\n",
    "ax.grid()\n",
    "ax.legend()\n",
    "\n",
    "sigma = 10.0\n",
    "f_vec_gauss_avg = filters.gaussian_filter1d(f_vec, sigma)\n",
    "ax = plt.subplot(2, 1, 2)\n",
    "ax.plot(time_vec, f_vec_gauss_avg, 'C0-', label=r\"$\\sigma = %.1f$\" % sigma)\n",
    "ax.set_xlabel(r\"$t$\")\n",
    "ax.set_ylabel(r\"data\")\n",
    "ax.set_ylim([2.7, 3.2])\n",
    "ax.grid()\n",
    "ax.legend()\n",
    "\n",
    "# Q: why is the latter more smooth?\n",
    "# Q: why does this not need time_vec to be reduced in size?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trends\n",
    "# most of the techniques we have played with in the previous workshops would work here too\n",
    "# we could for example do linear regression to pick out a linear trend\n",
    "\n",
    "# creating the artificial time-series again\n",
    "time_vec = np.arange(np.datetime64('2020-01-01'), np.datetime64('2021-12-31'), np.timedelta64(6, 'h'))\n",
    "nt = len(time_vec)\n",
    "t_vec = np.linspace(0, 2.0 * np.pi, nt)\n",
    "lin_trend = 0.05 * np.linspace(0, 2.0 * np.pi, nt)  # this is related to the answer we are looking for\n",
    "noise = 0.2 * np.random.rand(nt)\n",
    "f_vec = (  2.7                                      # this is related to the answer we are looking for\n",
    "         + 0.1 * np.sin(t_vec) \n",
    "         + 0.05 * np.sin(4.0 * t_vec) \n",
    "         + 0.02 * np.sin(60.0 * t_vec) \n",
    "         + lin_trend \n",
    "         + noise\n",
    "        )\n",
    "\n",
    "# could use the filtered one too, in this case probably doesn't matter\n",
    "f_vec_gauss_avg = filters.gaussian_filter1d(f_vec, 10)\n",
    "\n",
    "p_orig  = np.polyfit(t_vec, f_vec, 1)\n",
    "p_gauss = np.polyfit(t_vec, f_vec_gauss_avg, 1)\n",
    "\n",
    "fig = plt.figure(figsize=(10, 7))\n",
    "ax = plt.subplot(2, 1, 1)\n",
    "ax.plot(time_vec, f_vec)\n",
    "ax.plot(time_vec, p_orig[0] * t_vec + p_orig[1], 'k--',  # regressed linear trend\n",
    "        label=f\"${{{p_orig[0]:.3f}}} t + {{{p_orig[1]:.3f}}}$\")  \n",
    "ax.set_ylabel(r\"SST (${}^\\circ\\mathrm{C}$)\")\n",
    "ax.grid()\n",
    "ax.legend()\n",
    "\n",
    "ax = plt.subplot(2, 1, 2)\n",
    "ax.plot(time_vec, f_vec_gauss_avg)\n",
    "ax.plot(time_vec, p_gauss[0] * t_vec + p_gauss[1], 'k--',  # regressed linear trend\n",
    "        label=f\"${{{p_gauss[0]:.3f}}} t + {{{p_gauss[1]:.3f}}}$\")  \n",
    "ax.set_xlabel(r\"$t$ (years)\")\n",
    "ax.set_ylabel(r\"SST (${}^\\circ\\mathrm{C}$)\")\n",
    "ax.grid()\n",
    "ax.legend()\n",
    "\n",
    "# Q. the real second coefficient should be 2.7, so is the regression reproducing that?\n",
    "# Q. in the real linear trend we added the amplitude we gave was 0.05, but regression\n",
    "#    is returning a linear coefficient round 0.015\n",
    "#    the answer is actually correct, but what is the reason for the (apparent) discrepancy?\n",
    "#       hint: look at the t_vec window I used (not \"time_vec\", since this time-series depends solely on \"t_vec\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlations\n",
    "\n",
    "# so you know how to do calculate correlation coefficients from 03 and 04\n",
    "# here you might have two time-series and you want to see if they are correlated\n",
    "#                     the same time-series and if it is correlated with itself in some way\n",
    "\n",
    "# lets start with a very simple example where we should have a good feel for the answer before computing\n",
    "\n",
    "t_vec   = np.linspace(0, 2.0 * np.pi, 31)\n",
    "f       =   np.sin(t_vec)\n",
    "f_pos   = 2*np.sin(t_vec)\n",
    "f_neg   =  -np.sin(t_vec)\n",
    "f_shift =   np.sin(t_vec - np.pi / 2.0)\n",
    "\n",
    "fig = plt.figure(figsize=(5, 3))\n",
    "ax = plt.axes()\n",
    "ax.plot(t_vec, f    ,   \"C0\", label=\"f\")\n",
    "ax.plot(t_vec, f_pos,   \"C1\", label=\"f pos\") \n",
    "ax.plot(t_vec, f_neg,   \"C2\", label=\"f neg\")\n",
    "ax.plot(t_vec, f_shift, \"C3\", label=\"f shift\")\n",
    "ax.set_xlabel(r\"$t$\")\n",
    "ax.set_ylabel(r\"$f$\")\n",
    "ax.grid()\n",
    "ax.legend()\n",
    "\n",
    "# Q. what correlations do you expect relative to f?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the correlations\n",
    "# might be an idea to plot the correlations as a scatter graph\n",
    "\n",
    "fig = plt.figure(figsize=(10, 3))\n",
    "ax1 = plt.subplot(1, 3, 1)\n",
    "ax1.scatter(f, f_pos, color=\"C1\")\n",
    "ax1.set_xlabel(r\"f\")\n",
    "ax1.set_ylabel(r\"f pos\")\n",
    "ax1.grid()\n",
    "\n",
    "ax2 = plt.subplot(1, 3, 2)\n",
    "ax2.scatter(f, f_neg, color=\"C2\")\n",
    "ax2.set_xlabel(r\"f\")\n",
    "ax2.set_ylabel(r\"f neg\")\n",
    "ax2.grid()\n",
    "\n",
    "ax3 = plt.subplot(1, 3, 3)\n",
    "ax3.scatter(f, f_shift, color=\"C3\")\n",
    "ax3.set_xlabel(r\"f\")\n",
    "ax3.set_ylabel(r\"f shift\")\n",
    "ax3.grid()\n",
    "\n",
    "# so we should get 1, -1 and probably 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now lets see what the computation from package tells us\n",
    "# going to use scipy here, but scikit-learn would work (see 03)\n",
    "# (you can write one yourself too, you only need to covariance and the standard deviations of the data)\n",
    "\n",
    "_, _, r_pos, _, _ = stats.linregress(f, f_pos)\n",
    "_, _, r_neg, _, _ = stats.linregress(f, f_neg)\n",
    "_, _, r_shift, _, _ = stats.linregress(f, f_shift)\n",
    "\n",
    "print(f\"f and f_pos   has (linear/Pearson) correlation coefficient of {r_pos:.2f}\")\n",
    "print(f\"f and f_neg   has (linear/Pearson) correlation coefficient of {r_neg:.2f}\")\n",
    "print(f\"f and f_shift has (linear/Pearson) correlation coefficient of {r_shift:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross/lag correlation\n",
    "#\n",
    "# so lets go back to the shifted example but extend it a bit\n",
    "\n",
    "t_vec   = np.linspace(0, 4.0 * np.pi, 61)\n",
    "f       =   np.sin(t_vec)\n",
    "f_shift =   np.sin(t_vec - np.pi / 2.0)\n",
    "\n",
    "_, _, r_shift, _, _ = stats.linregress(f, f_shift)\n",
    "\n",
    "fig = plt.figure(figsize=(8, 3))\n",
    "ax = plt.axes()\n",
    "ax.plot(t_vec, f    ,   \"C0\", label=\"f\")\n",
    "ax.plot(t_vec, f_shift, \"C3\", label=\"f shift\")\n",
    "ax.set_xlabel(r\"$t$\")\n",
    "ax.set_ylabel(r\"$f$\")\n",
    "ax.grid()\n",
    "ax.legend()\n",
    "ax.set_title(f\"(linear/Pearson) correlation coefficient of {r_shift:.2f}\")\n",
    "\n",
    "# the corrleation coeffcient is zero, but from the graph it is clear that there are correlations between\n",
    "# the two signals (this is one of those cases where blindly applying statistics to data can give you a \n",
    "# misleading conclusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross/lag correlation\n",
    "#\n",
    "# in this example I cooked up the signal is just a shift, so what you might expect is that as I calculate\n",
    "# the correlation of the SHIFTED signal relative to each other, you would start to see correlation go up\n",
    "# eventually to 1\n",
    "\n",
    "# so, for example\n",
    "\n",
    "lag = 2  # lag by 5 indices\n",
    "\n",
    "# chop off the first few entries of the shift signal, last few entries of the original signal\n",
    "# (so that the two arrays are the same size and we can compute a correlation coefficient)\n",
    "_, _, r_lag, _, _ = stats.linregress(f[:-lag:], f_shift[lag::])\n",
    "\n",
    "fig = plt.figure(figsize=(8, 3))\n",
    "ax = plt.axes()\n",
    "ax.plot(t_vec[:-lag:], f      [:-lag:]    ,   \"C0\", label=\"f\")\n",
    "ax.plot(t_vec[:-lag:], f_shift[lag::], \"C3\", label=\"f shift with lag\")\n",
    "ax.plot(t_vec, f_shift, \"C3--\", label=\"f shift orig\", alpha=0.5)\n",
    "ax.set_xlabel(r\"$t$\")\n",
    "ax.set_ylabel(r\"$f$\")\n",
    "ax.grid()\n",
    "ax.legend()\n",
    "ax.set_title(f\"lag = {lag}, correlation coefficient of {r_lag:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross/lag correlation\n",
    "#\n",
    "# I can wrap this up in a subroutine and do this as a function of lag\n",
    "\n",
    "def custom_lag_corr(signal1, signal2, lag):\n",
    "    if len(signal1) != len(signal2):\n",
    "        raise Exception(\"array size not equal, cannot continue\")\n",
    "\n",
    "    if lag == 0:\n",
    "        _, _, r, _, _ = stats.linregress(signal1, signal2)\n",
    "    else:\n",
    "        _, _, r, _, _ = stats.linregress(signal1[:-lag:], signal2[lag::])\n",
    "    \n",
    "    return r\n",
    "\n",
    "n = 30\n",
    "r_lag = np.zeros(n)\n",
    "for lag in range(n):\n",
    "    r_lag[lag] = custom_lag_corr(f, f_shift, lag)\n",
    "\n",
    "fig = plt.figure(figsize=(8, 3))\n",
    "ax = plt.axes()\n",
    "ax.plot(np.arange(n), r_lag, \"C0-x\")\n",
    "ax.set_xlabel(r\"lag (in index)\")\n",
    "ax.set_ylabel(r\"$r$\")\n",
    "ax.grid()\n",
    "\n",
    "# if n is too big, the number of samples to calculate correlation gets low, so it increasingly becomes\n",
    "# a statistically dodgy manoeuvre (hence why I extend the signal a little bit more above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross/lag correlation\n",
    "\n",
    "# from the graph above you see that the signal has peak correlation at somewhere between lag index 7 and 8\n",
    "# (and the corresponding anti-correlation further down the line)\n",
    "# so in principle we could turn the lag index into a \"time\" (or whatever other measure you like \n",
    "# depending on context), essentially by finding out what \"lag = 1\" corresponds to in \"time\"\n",
    "\n",
    "# in this case I know the t_vec is uniform in time, so I just work out the differences and pick the first one\n",
    "# (if it isn't you have to associate each index with it's own time, which is doable but not touched on here)\n",
    "dt = np.diff(t_vec)[0]\n",
    "\n",
    "# again, I know the answer here: f_shift is shifted quarter wavelength out (by pi/2) relative to f\n",
    "#   so if I shift f_shift another quarter wavelength (  pi/2) then I should get maximum correlation\n",
    "#   so if I shift f_shift  three quarters wavelength (3 pi/2) then I should get minimum correlation\n",
    "\n",
    "fig = plt.figure(figsize=(8, 3))\n",
    "ax = plt.axes()\n",
    "ax.plot(np.arange(n) * dt, r_lag, \"C0-x\")\n",
    "ax.plot([np.pi / 2, np.pi / 2], [-2, 2], \"k--\", alpha=0.7)          # theoretical maximum\n",
    "ax.plot([3 * np.pi / 2, 3 * np.pi / 2], [-2, 2], \"k--\", alpha=0.7)  # theoreical minimum\n",
    "ax.set_xlabel(r\"lag (in time units)\")\n",
    "ax.set_ylabel(r\"$r$\")\n",
    "ax.set_ylim([-1.1, 1.1])\n",
    "ax.grid()\n",
    "\n",
    "# add the tick labels in\n",
    "xt = ax.get_xticks() \n",
    "xt = np.append(xt, [np.pi/2, 3*np.pi/2])\n",
    "xtl= xt.tolist()\n",
    "xtl[-2]=r\"$\\pi/2$\"\n",
    "xtl[-1]=r\"$3\\pi/2$\"\n",
    "ax.set_xticks(xt)\n",
    "ax.set_xticklabels(xtl)\n",
    "ax.set_xlim([0, 6]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# auto-correlation\n",
    "\n",
    "# if you can do lag correlations for two signals you could also do it for the signal with respect to itself\n",
    "#   (which, if you think about it, is what I actually did above...)\n",
    "# this is called the auto-correlation (correlation of signal with respect to lagged versions of itself)\n",
    "#\n",
    "# so you do this if you are interested to see how the signal correlates with itself, and whether you could\n",
    "#   use the signal's previous values to predict its future values\n",
    "\n",
    "# trivial example\n",
    "\n",
    "t_vec = np.linspace(0, 2.0 * np.pi, 31)\n",
    "f     = np.sin(t_vec)\n",
    "dt = np.diff(t_vec)[0]\n",
    "\n",
    "fig = plt.figure(figsize=(16, 3))\n",
    "\n",
    "n = 15\n",
    "r_lag = np.zeros(n)\n",
    "for lag in range(n):\n",
    "    r_lag[lag] = custom_lag_corr(f, f, lag)\n",
    "    \n",
    "ax = plt.subplot(1, 2, 1)\n",
    "ax.plot(np.arange(n) * dt, r_lag, \"C0-x\")\n",
    "ax.set_xlabel(r\"lag (in index)\")\n",
    "ax.set_ylabel(r\"$r$\")\n",
    "ax.set_title(f\"lag up to index {n}\")\n",
    "ax.grid()\n",
    "\n",
    "n = 30\n",
    "r_lag = np.zeros(n)\n",
    "for lag in range(n):\n",
    "    r_lag[lag] = custom_lag_corr(f, f, lag)\n",
    "    \n",
    "ax = plt.subplot(1, 2, 2)\n",
    "ax.plot(np.arange(n) * dt, r_lag, \"C0-x\")\n",
    "ax.set_xlabel(r\"lag (in index)\")\n",
    "ax.set_ylabel(r\"$r$\")\n",
    "ax.set_title(f\"lag up to index {n}\")\n",
    "ax.grid()\n",
    "\n",
    "# Q. is the left panel what you expect for the appropriate shifts\n",
    "# Q. given this is a sine/cosing curve, the correlations should be somewhat symmetric (see graph in cell above)\n",
    "#    but the right panel is not symmetric about the minimum point, why? (hint: what is the size of array?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# auto-correlation\n",
    "\n",
    "# and of course there is a package that someone coded up already we could have used\n",
    "# (the \"statsmodel\" module might not exist on your computers; you can download it yourself)\n",
    "# \n",
    "# anaconda: conda install -c conda-forge statsmodels\n",
    "# pip:      pip install statsmodels\n",
    "\n",
    "t_vec = np.linspace(0, 2.0 * np.pi, 31)\n",
    "f     = np.sin(t_vec)\n",
    "dt = np.diff(t_vec)[0]\n",
    "\n",
    "from statsmodels.graphics.tsaplots import plot_acf  # only gives the plotting\n",
    "# (or if you want access to the actual data etc., it is in \"statsmodels.tsa.stattools.acf\")\n",
    "\n",
    "fig = plt.figure(figsize=(10, 4))\n",
    "ax = plt.axes()\n",
    "title_str = f\"Auto-correlation, 1 lag index $\\leftrightarrow\\ dt = {dt:.3f}$\"\n",
    "plot_acf(f, ax=ax, lags=30, adjusted=True, title=title_str)\n",
    "ax.set_xlabel(f\"lag (index)\")\n",
    "ax.set_ylabel(f\"acf\")\n",
    "ax.grid()\n",
    "\n",
    "# this package gives you more information\n",
    "#    when \"adjusted=True\", the lines with dot ends are basically the same as above\n",
    "#    when \"adjusted=False\", it accounts for loss of data by the lag, and weights the auto-correlation accordingly\n",
    "#       -- so if you either leave out the argument or set it explicitly to False, the acf decays in time\n",
    "#    the envelope is the 95% confidence interval at alpha=0.05\n",
    "#       -- you can adjust alpha by providing e.g. \"alpha=0.01\" as a keyword\n",
    "#    when the dots lie within the confidence interval it is saying there is no strong statistical evidence\n",
    "#      to say values at the lag influence the current value (see 05 and 06)\n",
    "#       -- for this example it's saying if you know maybe 3 or 4 values then you can probably do a\n",
    "#          reasonable job predicting the next value\n",
    "#       -- remember this just says there is no strong statistical evidence, it doesn't mean there is no relation\n",
    "#          (in this artificial example there is in fact a strong relation but the statistics isn't picking it up)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now lets try to do something with more realistic data (El-Nino 3.4)\n",
    "# reading code below is just the same as above\n",
    "\n",
    "with open(\"elnino34_sst.data\", \"r\") as f:\n",
    "    elnino34_txt = f.readlines()\n",
    "elnino34_txt = elnino34_txt[3:-4]\n",
    "for k in range(len(elnino34_txt)):\n",
    "    elnino34_txt[k] = elnino34_txt[k].strip(\"\\n\")\n",
    "\n",
    "elnino34_txt[0].split()\n",
    "\n",
    "elnino34_sst = []\n",
    "for k in range(len(elnino34_txt)):           # this is the new elnino34_txt after stripping out some lines\n",
    "    dummy = elnino34_txt[k].split()          # split out the entries per line\n",
    "    for i in range(1, len(dummy)):           # cycle through the dummy list but skip the first entry\n",
    "        elnino34_sst.append(float(dummy[i])) # turn string into a float, then add to list\n",
    "\n",
    "elnino34_sst = np.array(elnino34_sst)\n",
    "\n",
    "# I want to do sums on this so I am going to use the raw version \n",
    "# (I personally find the numbers easier to manipulate)\n",
    "t_vec = np.linspace(1950, 2019+1, len(elnino34_sst), endpoint=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# El-Nino 3.4 linear trend\n",
    "# be careful here that time units are in YEARS\n",
    "p = np.polyfit(t_vec, elnino34_sst, 1)\n",
    "lin_trend = p[0] * t_vec + p[1]\n",
    "\n",
    "fig = plt.figure(figsize=(10, 3))\n",
    "ax = plt.axes()\n",
    "ax.plot(t_vec, elnino34_sst, 'C0')\n",
    "ax.plot(t_vec, lin_trend, 'k--')\n",
    "ax.text(1990, 24.5, f\"trend = ${p[0]:.3f}^{{\\circ}}\\ \\mathrm{{C}}$ per year\", color=\"k\")\n",
    "ax.set_xlabel(r\"$t$ (years)\")\n",
    "ax.set_ylabel(r\"SST (${}^{\\circ}\\mathrm{C}$)\")\n",
    "ax.set_ylim(24, 30)\n",
    "ax.grid()\n",
    "\n",
    "# Q. What does the trend mean here? Is this consistent with what is know? (you might need to look this up)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# El-Nino 3.4 filtering\n",
    "# if we are interested in the longer term oscillations, we might want to get rid of the higher\n",
    "# frequencies, so lets apply a filter to the signal\n",
    "\n",
    "fig = plt.figure(figsize=(10, 3))\n",
    "ax = plt.axes()\n",
    "\n",
    "sigma_vec = [2.0, 5.0, 10.0]\n",
    "\n",
    "for sigma in sigma_vec:\n",
    "    elnino34_gauss = filters.gaussian_filter1d(elnino34_sst, sigma)\n",
    "    ax.plot(t_vec, elnino34_gauss, label=f\"$\\sigma = {sigma}$\")\n",
    "ax.set_xlabel(r\"$t$ (years)\")\n",
    "ax.set_ylabel(r\"SST (${}^{\\circ}\\mathrm{C}$)\")\n",
    "ax.set_ylim(24, 30)\n",
    "ax.grid()\n",
    "ax.legend()\n",
    "\n",
    "# smaller sigma smooths out the signal a bit\n",
    "# larger sigma even gets rid of the shorter oscillations but keeps the longer ones (up to a point)\n",
    "#\n",
    "# Q. low pass the signal for a specified window of 6 months, 2 years and 10 years, and describe signal\n",
    "#    do this for both weighted and not weighted (e.g. tent kernel) options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# El-Nino 3.4 auto-correlation (unadjusted here)\n",
    "# when do the El-Nino 3.4 SST start decorrelating according to the auto-correlation analysis?\n",
    "\n",
    "dt = np.diff(t_vec)[0]\n",
    "\n",
    "from statsmodels.graphics.tsaplots import plot_acf  # only gives the plotting\n",
    "# (or if you want access to the actual data etc., it is in \"statsmodels.tsa.stattools.acf\")\n",
    "\n",
    "fig = plt.figure(figsize=(10, 4))\n",
    "ax = plt.axes()\n",
    "title_str = f\"Auto-correlation of El-Nino 3.4, 1 lag index $\\leftrightarrow\\ dt = 1$ month\"\n",
    "plot_acf(elnino34_sst, ax=ax, lags=24, title=title_str, alpha=0.05)\n",
    "ax.set_xlabel(f\"lag (index)\")\n",
    "ax.set_ylabel(f\"acf\")\n",
    "ax.grid()\n",
    "\n",
    "# Q. how do the acf's vary if you low-pass the signal (e.g. do you gain/lose \"predictability\")?\n",
    "# Q. (more involved) the acf above (probably?) computes the average acf over all points, \n",
    "#    so gives an average sense of how many previous points you need to reliably \"predict\" (?) the current point\n",
    "#    1) suppose you don't do that, and just compute the autocorrelation (look up the formula or a package)\n",
    "#       by only giving it truncated signal which has a date associated with it, do the results differ, and if so\n",
    "#       by how much? (i.e. the acf package gives you the average, but look into the samples itself)\n",
    "#    2) are some months more predictable?\n",
    "# Q. (more involved) the years with particularly large SSTs (being a bit vague here) are regarded as El-Nino years\n",
    "#    1) make up a way to pick these YEARS out from the time-series, and compare what the code returns with\n",
    "#       known El-Nino years\n",
    "#    2) are some El-Nino years \"more predictable\"? (i.e. do something like the previous Q.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# El-Nino 3.4 SST correlation with some other time-series data from this region\n",
    "# load some data diagnosed from the El-Nino 3.4 region (processed from data taken from copernicus.eu)\n",
    "# in this case this is also monthly data, but for averaged chlorophyll (mg/m3) from Jan 1993 to Dec 2020\n",
    "# (recall El-Nino 3.4 SST here is from Jan 1950 to Dec 2019)\n",
    "\n",
    "with open(\"elnino34_bgc.data\", \"r\") as f:\n",
    "    elnino34_txt = f.readlines()\n",
    "elnino34_txt = elnino34_txt[1::]  # strip out some unnecessary lines\n",
    "for k in range(len(elnino34_txt)):\n",
    "    elnino34_txt[k] = elnino34_txt[k].strip(\"\\n\")\n",
    "\n",
    "elnino34_chl = []\n",
    "for k in range(len(elnino34_txt)):\n",
    "    # split out the entries per line, pick out the 3rd entry, and strip out the floating comma, turn into float\n",
    "    elnino34_chl.append(float(elnino34_txt[k].split()[2].strip(\",\")))\n",
    "elnino34_chl = np.asarray(elnino34_chl)\n",
    "\n",
    "# create the analogous t_vec for this data\n",
    "t_vec_chl = np.linspace(1993, 2020+1, len(elnino34_chl), endpoint=False)\n",
    "\n",
    "# plot the SST and chlorophyll concentration out over the same time axis for comparison\n",
    "fig = plt.figure(figsize=(10, 7))\n",
    "ax = plt.subplot(2, 1, 1)\n",
    "ax.plot(t_vec, elnino34_sst)\n",
    "ax.set_ylabel(r\"SST (${}^\\circ\\mathrm{C}$)\")\n",
    "ax.set_xlim([1948, 2022])\n",
    "ax.grid()\n",
    "\n",
    "ax = plt.subplot(2, 1, 2)\n",
    "ax.plot(t_vec_chl, elnino34_chl, \"C2\")\n",
    "ax.set_xlabel(r\"$t$ (years)\")\n",
    "ax.set_ylabel(r\"chl-$a$ ($\\mathrm{mg}\\ \\mathrm{m}^3$)\")\n",
    "ax.set_xlim([1948, 2022])\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# El-Nino 3.4 SST correlation with some other time-series data from this region\n",
    "# pull out data from the same time window to compute cross-correlations\n",
    "\n",
    "sst_window = (t_vec >= 1993) & (t_vec <= 2019)\n",
    "chl_window = (t_vec_chl >= 1993) & (t_vec_chl <= 2019)\n",
    "\n",
    "fig = plt.figure(figsize=(10, 7))\n",
    "ax = plt.subplot(2, 1, 1)\n",
    "ax.plot(t_vec[sst_window], elnino34_sst[sst_window])\n",
    "ax.set_ylabel(r\"SST (${}^\\circ\\mathrm{C}$)\")\n",
    "ax.set_xlim([1990, 2022])\n",
    "ax.grid()\n",
    "\n",
    "ax = plt.subplot(2, 1, 2)\n",
    "ax.plot(t_vec_chl[chl_window], elnino34_chl[chl_window], \"C2\")\n",
    "ax.set_xlabel(r\"$t$ (years)\")\n",
    "ax.set_ylabel(r\"chl-$a$ ($\\mathrm{mg}\\ \\mathrm{m}^3$)\")\n",
    "ax.set_xlim([1990, 2022])\n",
    "ax.grid()\n",
    "\n",
    "# linear regression to get the correlation coefficient\n",
    "_, _, r, _, _ = stats.linregress(elnino34_sst[sst_window], elnino34_chl[chl_window])\n",
    "\n",
    "print(f\"cross correlation (un-lagged) between SST and chl-a is {r:.6f}\")\n",
    "\n",
    "# Q. interpret the correlation\n",
    "# Q. try a lag analysis on these signals accordingly and see if anything interesting comes out\n",
    "# Q. try the above, but for low-passed signals (explore the choice of time window)\n",
    "# Q. (harder) are the relevant lag correlations (if any) consistent with physical rationale for El-Nino,\n",
    "#             particularly during El-Nino years? (you may have to look up which ones these are)\n",
    "# Q. (involved) the dataset just loaded also includes phytoplankton concentration, try and do the analogous\n",
    "#               analyses for the various pairs of data (SST, chl-a, phytoplankton)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q. (involved) the code here is dirty (no apologies for that actually) and relies a lot on native python commands,\n",
    "#               particularly when dates are involved etc. Try do the same thing thus far but using pandas \n",
    "#               (which would be much cleaner for managing the data) and python packages. \n",
    "#\n",
    "#       Several things you might want to do:\n",
    "#          1) put everything into one pandas dataframe so there aren't multiple arrays hanging around\n",
    "#          2) there is a time-mismatch, so to have only one time dimension, either\n",
    "#             i ) get rid of some data in SST\n",
    "#             ii) fill out the shorter array with NaNs or missing values\n",
    "#          3) there are some commands in pandas that might be useful \n",
    "#             e.g. .mean, .sum, .rolling, etc., look these up on Google"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
